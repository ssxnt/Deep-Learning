{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cjJJR8DnOKn"
      },
      "source": [
        "# Programming Assignment 2: Variational Autoencoder\n",
        "\n",
        "**UBC CPEN 455: Deep Learning, 2024 Winter Term 2**\n",
        "\n",
        "**Created By Qi Yan, Renjie Liao**\n",
        "\n",
        "**Date: Mar. 20, 2025**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw_aVg5OnOKq"
      },
      "source": [
        "## Preface\n",
        "In this assignment, you will implement a VAE model from scratch. A Colab notebook is provided with this documentation, which you need to complete. Please pay attention to the following notes:\n",
        "\n",
        "* You are only required to modify and fill in the code inside the START/END blocks.\n",
        "* For each module, one test case is provided. You may use the test cases to get a sense of how the input/output formats are, and as a sanity check for your implementation.\n",
        "* Make sure your code is readable (by leaving comments and writing self-commented code).\n",
        "* Unless otherwise specified, you are not allowed to use any module from `torch.distributions` module.\n",
        "* All the code must be written by yourself, and you are not allowed to copy any code from other sources such as your classmates or the internet.\n",
        "* If you use ChatGPT or other LLMs to help finish the assignment, please clearly mark which questions you use them. We may require you to submit your prompts. Please make sure you store them properly.\n",
        "* Your code should support training on both CPU and CUDA devices, depending on availability (with CUDA prioritized). Not supporting GPU training will result in a loss of points.\n",
        "* Very slow training will also lead to a loss of points. Your implementation should lead to efficient training that completes within 5 minutes; otherwise, you will lose points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKDB2KvOnOKq"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BLNk41ngsF4S"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import random\n",
        "import tarfile, requests, os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from tqdm.notebook import tqdm, trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4J10uX_Ps0dl"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=2025, mute=True):\n",
        "  \"\"\"\n",
        "  Seet randomseed to control randomness.\n",
        "  \"\"\"\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  if not mute:\n",
        "    print(f'Random seed {seed} has been set.')\n",
        "\n",
        "def set_device():\n",
        "  \"\"\"\n",
        "  Set the device on which to run PyTorch. CUDA if available, CPU otherwise\n",
        "  \"\"\"\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "    print(\"WARNING: For this notebook to perform best, please enable GPU.\")\n",
        "  else:\n",
        "    print(\"GPU is enabled in this notebook.\")\n",
        "\n",
        "  return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tM-KNKedsw-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cba14adf-c943-428b-d4a4-48b54a843829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed 2025 has been set.\n",
            "WARNING: For this notebook to perform best, please enable GPU.\n"
          ]
        }
      ],
      "source": [
        "SEED=2025\n",
        "set_seed(seed=SEED, mute=False)\n",
        "DEVICE = set_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Sixs1kb0sYih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e962bdf9-6039-467b-d7cb-485feabbf66e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST dataset has been downloaded.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download MNIST dataset\n",
        "fname = 'MNIST.tar.gz'\n",
        "name = 'mnist'\n",
        "url = 'https://osf.io/y2fj6/download'\n",
        "\n",
        "if not os.path.exists(name):\n",
        "  print('\\nDownloading MNIST dataset...')\n",
        "  r = requests.get(url, allow_redirects=True)\n",
        "  with open(fname, 'wb') as fh:\n",
        "    fh.write(r.content)\n",
        "  print('\\nDownloading MNIST completed!\\n')\n",
        "\n",
        "if not os.path.exists(name):\n",
        "  with tarfile.open(fname) as tar:\n",
        "    tar.extractall(name)\n",
        "    os.remove(fname)\n",
        "else:\n",
        "  print('MNIST dataset has been downloaded.\\n')\n",
        "\n",
        "# Load MNIST image datasets\n",
        "# See https://pytorch.org/docs/stable/torchvision/datasets.html\n",
        "train_set = datasets.MNIST('./mnist/',\n",
        "                       train=True,\n",
        "                       transform=transforms.ToTensor(),\n",
        "                       download=False)\n",
        "data_shape = (1, 28, 28)\n",
        "data_size = 28 * 28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FFVb8VCispri"
      },
      "outputs": [],
      "source": [
        "def plot_torch_image(image, ax=None):\n",
        "  \"\"\"\n",
        "  Helper function to plot torch image\n",
        "  @param image: torch.Tensor\n",
        "  @param ax: plt object\n",
        "  \"\"\"\n",
        "  ax = ax if ax is not None else plt.gca()\n",
        "  c, h, w = image.size()\n",
        "  if c==1:\n",
        "    cm = 'gray'\n",
        "  else:\n",
        "    cm = None\n",
        "\n",
        "  # [C, H, W] -> [H, W, C]\n",
        "  im_plt = torch.clip(image.detach().cpu().permute(1,2,0).squeeze(), 0.0, 1.0)\n",
        "  ax.imshow(im_plt, cmap=cm)\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "\n",
        "\n",
        "def plot_images(images, h=3, w=3, plt_title=''):\n",
        "  \"\"\"\n",
        "  Helper function to plot images.\n",
        "  @param images: torch.Tensor\n",
        "  @param h: int, number of images in height direction\n",
        "  @param w: int, number of images in width direction\n",
        "  @param plt_title: string, Plot title\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(h*2, w*2))\n",
        "  plt.suptitle(plt_title)\n",
        "  for i in range(h*w):\n",
        "    plt.subplot(h, w, i + 1)\n",
        "    plot_torch_image(images[i])\n",
        "\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUGqwxWrnOKu"
      },
      "source": [
        "## Implement a variational eutoendocer\n",
        "\n",
        "The Variational Autoencoder (VAE) model defines a class of generative models based on latent space variables. In a VAE, we have an encoder that maps inputs $\\mathbf{x}$ (e.g., image data) to entire distributions over hidden vectors $\\mathbf{z}$, denoted by:\n",
        "\n",
        "$$\\mathbf{x} \\overset{\\text{VAE}}{\\longrightarrow} q_{\\mathbf{w_e}}(\\mathbf{z})$$\n",
        "\n",
        "We then sample from this distribution. Here, $\\mathbf{w_e}$ refers to the weights of the encoder, which parameterize the distribution generating network. The specific form of the distribution $q_{\\mathbf{w_e}}(\\mathbf{z})$ is described in the following section.\n",
        "\n",
        "In the decoder, reconstructions are expressed in terms of a distribution:\n",
        "\n",
        "$$\\mathbf{z} \\overset{\\text{VAE}}{\\longrightarrow} p_{\\mathbf{w_d}}(\\mathbf{x}|\\mathbf{z})$$\n",
        "\n",
        "As above, $p_{\\mathbf{w_d}}(\\mathbf{x}|\\mathbf{z})$ is defined by mapping $\\mathbf{z}$ through a density network, and then treating the resulting $f(\\mathbf{z};\\mathbf{w_d})$ as the mean of a Gaussian distribution over $\\mathbf{x}$. Similarly, the reconstruction distribution is parameterized by the weights of the density network.\n",
        "\n",
        "To be specific, we use the evidence lower bound (ELBO) loss to train a VAE, which is composed of reconstruction loss and regularization loss:\n",
        "\n",
        "$$\\mathcal{L}_{\\text{ELBO}} = -\\mathbb{E}_{q(\\mathbf{z} \\mid \\mathbf{x})} \\left[\\log p(\\mathbf{x} \\mid \\mathbf{z}) \\right] + \\text{KL}\\left(q(\\mathbf{z} \\mid \\mathbf{x}) \\| p(\\mathbf{z}) \\right)$$\n",
        "\n",
        "Here, $q$ and $p$ are the encoder and decoder, respectively. We drop the subscripts $\\mathbf{w_e}$ and $\\mathbf{w_d}$ for brevity. The loss function of the VAE requires good reconstructions of the input not just for a single $\\mathbf{z}$, but on average from samples of $\\mathbf{z} \\sim q_{\\mathbf{w_e}}(\\mathbf{z})$.\n",
        "\n",
        "In the following sections, we will first implement each component of the ELBO loss, and then train and use VAEs to generate image samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnZY8tMUnOKu"
      },
      "source": [
        "## Q1.1 [5 Pts]: Implement Gaussian likelihood computation\n",
        "\n",
        "In this task, we need to compute the log likelihood of the reconstructed data assuming a $k$-dimension isotropic Gaussian distribution with mean $\\boldsymbol{\\mu}$ and standard deviation $\\sigma$.\n",
        "\n",
        "The PDF of the isotropic Gaussian distribution is given by:\n",
        "\n",
        "\\begin{equation*}\n",
        "p(\\mathbf{x} \\mid \\boldsymbol{\\mu},\\sigma^2\\mathbf{I}) = \\frac{1}{(2\\pi)^{k/2}\\sigma^k} \\exp\\left(-\\frac{1}{2\\sigma^2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\top(\\mathbf{x}-\\boldsymbol{\\mu})\\right)\n",
        "\\end{equation*}\n",
        "\n",
        "where $\\mathbf{x}$ is the observed sample.\n",
        "\n",
        "Note that the isotropic Gaussian has a covariance matrix of the form $\\Sigma = \\sigma^2\\mathbf{I}$, where $\\mathbf{I}$ is the identity matrix of size $k$. This means that there is only one parameter governing the covariance matrix, which can be represented as $\\sigma$. To ensure numerical stability, we usually parameterize the covariance using $\\log \\sigma$.\n",
        "\n",
        "**Task**: Implement a function that takes the mean and standard deviation of the Gaussian distribution, along with the observed sample, and computes the log-likelihood of the sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "oex0QHHTtk4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc054bf-6698-4bac-ddfd-35c351f4181c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your implementation is correct.\n"
          ]
        }
      ],
      "source": [
        "from logging import log\n",
        "def log_prob(x: torch.Tensor, mu: torch.Tensor, log_sigma: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    ### Fill in this function ###\n",
        "    Compute element-wise log Gaussian probability.\n",
        "    @param x:          [B, N, K]: input data (observations of Gaussian distribution), there are N samples for each of the B batches, each sample has K dimensions.\n",
        "    @param mu:         [B, N, K]: mean of Gaussian distribution.\n",
        "    @param log_sigma:  [B, N]: log of standard deviation of Gaussian distribution.\n",
        "    @return log_prob:  [B, N]: log Gaussian probability for each sample in the batch.\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # this is essentially the same as...\n",
        "    # log_prob = -0.5 * k * log(2π) - k * log_sigma - 0.5 * squared_diff * exp(-2 * log_sigma)\n",
        "    log_prob = (-0.5*x.shape[2]*torch.log(torch.tensor(2*torch.pi)))-(x.shape[2]*log_sigma)-(0.5*torch.sum((x-mu)**2, dim=2)*torch.exp(-2*log_sigma))\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return log_prob\n",
        "\n",
        "\n",
        "def gt_log_prob(x: torch.Tensor, mu: torch.Tensor, log_sigma: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Ground truth function used for unit test. Do not modify.\n",
        "    @param x:          [B, N, K]: input data (observations of Gaussian distribution), there are N samples for each of the B batches, each sample has K dimensions.\n",
        "    @param mu:         [B, N, K]: mean of Gaussian distribution.\n",
        "    @param log_sigma:  [B, N]: log of standard deviation of Gaussian distribution.\n",
        "    @return log_prob:  [B, N]: log Gaussian probability for each sample in the batch.\n",
        "    \"\"\"\n",
        "    gt_log_prob = torch.distributions.normal.Normal(mu, log_sigma[:, :, None].exp()).log_prob(x)\n",
        "    gt_log_prob = gt_log_prob.sum(dim=-1)\n",
        "    return gt_log_prob\n",
        "\n",
        "\n",
        "def unit_test_log_prob(func_log_prob=log_prob, func_gt_log_prob=gt_log_prob):\n",
        "    \"\"\"\n",
        "    Unit test helper. Do not modify.\n",
        "    \"\"\"\n",
        "    b, n = torch.randint(1, 32, [2])\n",
        "    k = torch.randint(1, 8, [1])\n",
        "    x = torch.randn([b, n, k])\n",
        "    mu = torch.randn([b, n, k])\n",
        "    log_sigma = torch.rand([b, n])\n",
        "\n",
        "    true_log_prob = func_gt_log_prob(x, mu, log_sigma)                  # [B, N]\n",
        "    compute_log_prob = func_log_prob(x, mu, log_sigma)                  # [B, N]\n",
        "    error = (true_log_prob - compute_log_prob).square().mean().sqrt()   # scalar\n",
        "\n",
        "    if error < 1e-5:\n",
        "        print(\"Your implementation is correct.\")\n",
        "    else:\n",
        "        print(\"Your implementation is incorrect.\")\n",
        "\n",
        "\n",
        "set_seed(seed=SEED)\n",
        "unit_test_log_prob()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4S-UFSGnOKu"
      },
      "source": [
        "## Q1.2 [5 Pts]: Implement Monte-Carlo Approximation of KL Divergence\n",
        "\n",
        "The KL divergence between two distributions $q$ and $p$ is defined as:\n",
        "\n",
        "\\begin{equation*}\n",
        "KL(q \\parallel p) = \\int q(x)\\log \\frac{q(x)}{p(x)} = \\mathbb{E}_{x\\sim q}\\left[\\log \\frac{q(x)}{p(x)} \\right] = \\mathbb{E}_{x\\sim q}\\left[ \\log q(x) - \\log p(x) \\right]\n",
        "\\end{equation*}\n",
        "\n",
        "In practice, we can approximate it using Monte Carlo estimation. Suppose we have two sets of $N$ i.i.d. samples $x=\\{x^{(1)}, x^{(2)}, \\cdots, x^{(N)}\\}$ from the $q$ distribution, and we can compute their probability densities under $q$ and $p$. Then, we can estimate the rightmost expectation term with:\n",
        "\n",
        "\\begin{equation*}\n",
        "KL(q \\parallel p) \\approx \\frac{1}{N}\\sum_{i=1}^N\\left(\\log q(x^{(i)}) - \\log p(x^{(i)})\\right)\n",
        "\\end{equation*}\n",
        "\n",
        "Please refer to http://joschu.net/blog/kl-approx.html for more information.\n",
        "\n",
        "**Task**: Assume $q$ and $p$ are multivariate Gaussian distributions with isotropic covariance matrix (as in Q1.1). Given samples from $q$ distributions, and the mean and standard deviation of $q$ and $p$, compute the KL divergence using Monte Carlo estimation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "3w3bK3nfnOKv",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "655d87ea-1f38-48a2-9bb7-c10223d3b82a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your implementation is correct.\n"
          ]
        }
      ],
      "source": [
        "def kl_q_p_mc(samples_q: torch.Tensor, params_q: torch.Tensor, params_p: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    ### Fill in this function ###\n",
        "    Compute the KL divergence between the q distirubtion and p distribution using Monte-Carlo approximation.\n",
        "    Hints: you could reuse the above log_prob function.\n",
        "    @param samples_q: [B, N, K]: samples from B many q distributions, there are N samples in each batch, each of them has K dimensions\n",
        "    @param params_q: [B, K+1]: parameters of B many q distribution, the first K elements are the mean, the last element is the log standard deviation\n",
        "    @param params_p: [B, K+1]: parameters of B many p distribution, the first K elements are the mean, the last element is the log standard deviation\n",
        "    @return kl_q_p_mc [B]: KL(q||p) for q||p\n",
        "    \"\"\"\n",
        "    # Init\n",
        "    b, n, k = samples_q.size()\n",
        "    mu_q, log_sig_q = params_q[:, :-1], params_q[:, -1]  # [B, K], [B]\n",
        "    mu_p, log_sig_p = params_p[:, :-1], params_p[:, -1]  # [B, K], [B]\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    mu_q_new = mu_q.unsqueeze(1).expand(-1, n, -1) # i have to make these sizes match my log_prob() implementation above\n",
        "    mu_p_new = mu_p.unsqueeze(1).expand(-1, n, -1) # now it's [b,n,k]\n",
        "\n",
        "    log_sig_q_new = log_sig_q.unsqueeze(1).expand(-1, n) # refer previous comment\n",
        "    log_sig_p_new = log_sig_p.unsqueeze(1).expand(-1, n) # this is now [b,n]\n",
        "\n",
        "    logp = log_prob(samples_q, mu_p_new, log_sig_p_new)\n",
        "    logq = log_prob(samples_q, mu_q_new, log_sig_q_new)\n",
        "\n",
        "    kl_mc = (logq-logp).mean(dim=1)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return kl_mc\n",
        "\n",
        "\n",
        "def gt_kl_q_p_mc(samples_q: torch.Tensor, params_q: torch.Tensor, params_p: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Ground truth function used for unit test. Do not modify.\n",
        "    @param samples_q: [B, N, K]: samples from B many q distributions, there are N samples in each batch, each of them has K dimensions\n",
        "    @param params_q: [B, K+1]: parameters of B many q distribution, the first K elements are the mean, the last element is the log standard deviation\n",
        "    @param params_p: [B, K+1]: parameters of B many p distribution, the first K elements are the mean, the last element is the log standard deviation\n",
        "    @return kl_q_p_mc [B]: KL(q||p) for q||p\n",
        "    \"\"\"\n",
        "    # Init\n",
        "    b, n, k = samples_q.size()\n",
        "    mu_q, log_sig_q = params_q[:, :-1], params_q[:, -1]  # [B, K], [B]\n",
        "    mu_p, log_sig_p = params_p[:, :-1], params_p[:, -1]  # [B, K], [B]\n",
        "    sigma_q, sigma_p = log_sig_q.exp(), log_sig_p.exp()  # [B], [B]\n",
        "\n",
        "    gt_log_prob_q = torch.distributions.normal.Normal(mu_q.unsqueeze(1), sigma_q[:, None, None]).log_prob(samples_q)  # [B, N, K]\n",
        "    gt_log_prob_q = gt_log_prob_q.sum(dim=-1)  # [B]\n",
        "    gt_log_prob_p = torch.distributions.normal.Normal(mu_p.unsqueeze(1), sigma_p[:, None, None]).log_prob(samples_q)  # [B, N, K]\n",
        "    gt_log_prob_p = gt_log_prob_p.sum(dim=-1)  # [B]\n",
        "    return (gt_log_prob_q - gt_log_prob_p).mean(dim=1)  # [B]\n",
        "\n",
        "\n",
        "def unit_test_kl_q_p_mc(func_kl_q_p_mc=kl_q_p_mc, func_gt_kl_q_p_mc=gt_kl_q_p_mc):\n",
        "    \"\"\"\n",
        "    Unit test helper. Do not modify.\n",
        "    \"\"\"\n",
        "    b, n = torch.randint(1, 32, [2])\n",
        "    k = torch.randint(1, 8, [1])\n",
        "    x_q = torch.randn([b, n, k])\n",
        "    mu_q = torch.randn([b, k])\n",
        "    log_sigma_q = torch.rand([b])\n",
        "\n",
        "    mu_p = torch.randn([b, k])\n",
        "    log_sigma_p = torch.rand([b])\n",
        "\n",
        "    params_q = torch.cat([mu_q, log_sigma_q[:, None]], dim=-1)            # K means + 1 log sigma\n",
        "    params_p = torch.cat([mu_p, log_sigma_p[:, None]], dim=-1)            # K means + 1 log sigma\n",
        "\n",
        "    true_kl_q_p_mc =  func_gt_kl_q_p_mc(x_q, params_q, params_p)     # [B]\n",
        "    compute_kl_q_p_mc = func_kl_q_p_mc(x_q, params_q, params_p)      # [B]\n",
        "    error = (true_kl_q_p_mc - compute_kl_q_p_mc).square().mean().sqrt()   # scalar\n",
        "    if error < 1e-5:\n",
        "        print(\"Your implementation is correct.\")\n",
        "    else:\n",
        "        print(\"Your implementation is incorrect.\")\n",
        "\n",
        "\n",
        "set_seed(seed=SEED)\n",
        "unit_test_kl_q_p_mc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D34mx_O-nOKv"
      },
      "source": [
        "## Q1.3 [5 Pts]: Implement exact KL divergence\n",
        "In Q1.2, we assumed a simple case where both the $q$ and $p$ distributions were Gaussian and their parameters were known. More precisely, we could evaluate the exact KL divergence between them using the following formula:\n",
        "\n",
        "\\begin{equation*}\n",
        "KL(\\mathcal{N}_q\\parallel\\mathcal{N}_p) = \\frac{1}{2}\\left(k \\left(\\frac{\\sigma_q}{\\sigma_p}\\right)^2 + (\\boldsymbol{\\mu}_q - \\boldsymbol{\\mu}_p)^\\top \\frac{1}{\\sigma_p^2} (\\boldsymbol{\\mu}_q - \\boldsymbol{\\mu}_p) - k + 2 k \\log\\frac{\\sigma_p}{\\sigma_q}\\right)\n",
        "\\end{equation*}\n",
        "\n",
        "For more information about the derivation, see this [reference](https://mr-easy.github.io/2020-04-16-kl-divergence-between-2-gaussian-distributions/).\n",
        "\n",
        "**Task**: Assuming that $q$ and $p$ are multivariate Gaussian distributions with isotropic covariance matrices (as in Q1.1), and given the mean and standard deviation of $q$ and $p$, compute the exact KL divergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ORD0EdyinOKv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95925dc9-0add-4345-f37d-d36e9b9090e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your implementation is correct.\n"
          ]
        }
      ],
      "source": [
        "def kl_q_p_exact(params_q: torch.Tensor, params_p: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    ### Fill in this function ###\n",
        "    Compute the exact KL divergence between the q distirubtion and p distribution.\n",
        "    Hints: you could reuse the above log_prob function.\n",
        "    @param params_q: [B, K+1]: parameters of B many q distribution, the first K elements are the mean, the last element is the log standard deviation\n",
        "    @param params_p: [B, K+1]: parameters of B many p distribution, the first K elements are the mean, the last element is the log standard deviation\n",
        "    @return kl_q_p_e [B]: KL(q||p) for q||p\n",
        "    \"\"\"\n",
        "    # Init\n",
        "    b, k_ = params_q.shape\n",
        "    k = k_ - 1\n",
        "    mu_q, log_sig_q = params_q[:, :-1], params_q[:, -1]  # [B, K], [B]\n",
        "    mu_p, log_sig_p = params_p[:, :-1], params_p[:, -1]  # [B, K], [B]\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    sigma_q = log_sig_q.exp()\n",
        "    sigma_p = log_sig_p.exp()\n",
        "\n",
        "    term1 = 0.5*k*(sigma_q/sigma_p)**2\n",
        "    term2 = 0.5*torch.sum((mu_q-mu_p)**2, dim=1)/(sigma_p**2)-0.5*k\n",
        "    term3 = 0.5*2*k*torch.log(sigma_p/sigma_q)\n",
        "\n",
        "    kl = term1+term2+term3\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return kl\n",
        "\n",
        "\n",
        "def gt_kl_q_p_exact(params_q: torch.Tensor, params_p: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Ground truth function used for unit test. Do not modify.\n",
        "    @param params_q: [B, K+1]: parameters of B many q distribution, the first K elements are the mean, the last element is the log standard deviation\n",
        "    @param params_p: [B, K+1]: parameters of B many p distribution, the first K elements are the mean, the last element is the log standard deviation\n",
        "    @return kl_q_p_e [B]: KL(q||p) for q||p\n",
        "    \"\"\"\n",
        "    # Init\n",
        "    mu_q, log_sig_q = params_q[:, :-1], params_q[:, -1]  # [B, K], [B]\n",
        "    mu_p, log_sig_p = params_p[:, :-1], params_p[:, -1]  # [B, K], [B]\n",
        "\n",
        "    gt_log_prob_q = torch.distributions.normal.Normal(mu_q, log_sig_q[:, None].exp())\n",
        "    gt_log_prob_p = torch.distributions.normal.Normal(mu_p, log_sig_p[:, None].exp())\n",
        "    gt_kl_q_p_exact = torch.distributions.kl.kl_divergence(gt_log_prob_q, gt_log_prob_p).sum(dim=-1)  # [B] <- [B, K]\n",
        "    return gt_kl_q_p_exact  # [B]\n",
        "\n",
        "\n",
        "def unit_test_kl_q_p_exact(func_kl_q_p_exact=kl_q_p_exact, func_gt_kl_q_p_exact=gt_kl_q_p_exact):\n",
        "    \"\"\"\n",
        "    Unit test helper. Do not modify.\n",
        "    \"\"\"\n",
        "    b = torch.randint(1, 32, [1])\n",
        "    k = torch.randint(1, 8, [1])\n",
        "    mu_q = torch.randn([b, k])\n",
        "    log_sigma_q = torch.rand([b])\n",
        "    mu_p = torch.randn([b, k])\n",
        "    log_sigma_p = torch.rand([b])\n",
        "\n",
        "    params_q = torch.cat([mu_q, log_sigma_q[:, None]], dim=-1)\n",
        "    params_p = torch.cat([mu_p, log_sigma_p[:, None]], dim=-1)\n",
        "\n",
        "    compute_kl_q_p_exact = func_kl_q_p_exact(params_q, params_p)  # [B]\n",
        "    true_kl_q_p_exact = func_gt_kl_q_p_exact(params_q, params_p)  # [B]\n",
        "\n",
        "    error = (true_kl_q_p_exact - compute_kl_q_p_exact).square().mean().sqrt()  # scalar\n",
        "    if error < 1e-5:\n",
        "        print(\"Your implementation is correct.\")\n",
        "    else:\n",
        "        print(\"Your implementation is incorrect.\")\n",
        "        print(compute_kl_q_p_exact)\n",
        "        print(true_kl_q_p_exact)\n",
        "\n",
        "\n",
        "set_seed(seed=SEED)\n",
        "unit_test_kl_q_p_exact()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXjTaqWlnOKv"
      },
      "source": [
        "### Compare Monte-Carlo Approximation and Exact KL Divergence\n",
        "\n",
        "*Note: No implementation is required.*\n",
        "\n",
        "In Q1.2 and Q1.3, we have implemented two methods to estimate the KL divergence between two distributions. The Monte-Carlo estimation method provides an approximation to the KL divergence by drawing samples from the $q$ distributions. On the other hand, the exact KL divergence formula can be used when the two distributions are known.\n",
        "\n",
        "In practice, we can compare the two methods to see how well the Monte-Carlo approximation performs. When a sufficient number of samples are used, the Monte-Carlo method can yield results that are close to the exact KL divergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mi74xQfAnOKv"
      },
      "outputs": [],
      "source": [
        "def compare_exact_mc_kl(func_gt_kl_q_p_exact=gt_kl_q_p_exact, func_gt_kl_q_p_mc=gt_kl_q_p_mc):\n",
        "    \"\"\"\n",
        "    Unit test helper. Do not modify.\n",
        "    \"\"\"\n",
        "    b, k = 1, 4\n",
        "    mu_q = torch.zeros([b, k])\n",
        "    log_sigma_q = torch.zeros([b])\n",
        "    mu_p = torch.ones([b, k]) * 10.0\n",
        "    log_sigma_p = torch.ones([b])\n",
        "\n",
        "    params_q = torch.cat([mu_q, log_sigma_q[:, None]], dim=-1)\n",
        "    params_p = torch.cat([mu_p, log_sigma_p[:, None]], dim=-1)\n",
        "\n",
        "    true_kl_q_p_exact = func_gt_kl_q_p_exact(params_q, params_p)  # [B]\n",
        "\n",
        "    num_samples = torch.linspace(1, 1000, 20).clamp(min=1.0).long()\n",
        "    num_repeat = 10\n",
        "\n",
        "    diff_ls, var_ls = [], []\n",
        "    for n in num_samples:\n",
        "        this_diff = []\n",
        "        for r in range(num_repeat):\n",
        "            x_q = torch.distributions.normal.Normal(mu_q, log_sigma_q[:, None].exp()).sample([n])  # [N, B, K]\n",
        "            x_q = x_q.permute(1, 0, 2)  # [B, N, K]\n",
        "            true_gt_kl_q_p_mc = func_gt_kl_q_p_mc(x_q, params_q, params_p)  # [B]\n",
        "\n",
        "            this_diff.append((true_kl_q_p_exact - true_gt_kl_q_p_mc).norm(p=2).item())  # scalar\n",
        "\n",
        "        diff_ls.append(np.mean(this_diff))  # scalar\n",
        "        var_ls.append(np.var(this_diff))  # scalar\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.gca()\n",
        "    # ax.scatter(num_samples.numpy(), diff_ls)\n",
        "    ax.bar(num_samples.numpy(), diff_ls, width=20.0, yerr=var_ls, capsize=5.0, color='blue', alpha=0.5)\n",
        "    ax.set_xlabel(\"Number of samples\")\n",
        "    ax.set_ylabel(\"Mean squared error \\nbetween MC and exact KL divergence\")\n",
        "    # ax.set_xscale('log')\n",
        "    ax.set_yscale('log')\n",
        "    ax.set_title(\"MC KL vs exact KL divergence\\n we repeat 10 runs for each MC KL estimation\")\n",
        "\n",
        "    # print(diff_ls)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "set_seed(seed=SEED)\n",
        "compare_exact_mc_kl()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbgue3XTnOKv"
      },
      "source": [
        "## Q1.4 [5 Pts]: Implement reparameterization trick for sampling\n",
        "\n",
        "The reparameterization trick is a method that allows us to sample from a distribution in a way that makes backpropagation of gradients through the sampling operation possible. The basic idea is to transform the random variable to be sampled into a function of a random variable with fixed distribution, and then sample from the fixed distribution. For example, we can use this trick to get samples from a Gaussian distribution without \"directly drawing samples\" from it.\n",
        "\n",
        "Suppose we want to sample from a $k$-dimensional Gaussian distribution with isotropic covariance matrix. Let $\\boldsymbol{\\mu} \\in \\mathbb{R}^k$ and $\\sigma \\in \\mathbb{R}$ be the mean and standard deviation of the distribution, respectively. We can reparameterize a random variable $\\boldsymbol{z} \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{I})$ as $\\boldsymbol{x} = \\boldsymbol{\\mu} + \\sigma\\boldsymbol{z}$ to obtain a sample from the Gaussian distribution. Here, $\\boldsymbol{z}$ is a $k$-dimensional vector of independent standard normal random variables, and $\\boldsymbol{I}$ is the $k \\times k$ identity matrix. During sampling, we first get samples from $\\boldsymbol{z}$, and transform them linearly to get samples from $\\boldsymbol{x}$.\n",
        "\n",
        "**Task**: Given the mean and standard deviation of a $k$-dimension Gaussian distribution with isotropic covariance matrix, implement the reparameterization trick to draw samples from this distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HHOXOhCnOKw"
      },
      "outputs": [],
      "source": [
        "def rsample(params_q: torch.Tensor, num_samples: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    ### Fill in this function ###\n",
        "    Obtain samples from Gaussian distributions, provided the mean and covariance parameters.\n",
        "    @param params_q:        [B, K+1]: parameters of B many q distribution, the first K elements are the mean, the last element is the log standard deviation\n",
        "    @param num_samples:     int: number of samples to create\n",
        "    @return x_q:            [B, N, K]: samples from q distribution, where N is the number of samples\n",
        "    \"\"\"\n",
        "\n",
        "    # Init\n",
        "    b, k_ = params_q.shape\n",
        "    k = k_ - 1\n",
        "    mu_q, log_sig_q = params_q[:, :-1], params_q[:, -1]  # [B, K], [B]\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return x_q\n",
        "\n",
        "\n",
        "def gt_rsample(params_q: torch.Tensor, num_samples: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Ground truth implementation of rsample. Do not modify.\n",
        "    @param params_q:        [B, K+1]: parameters of B many q distribution, the first K elements are the mean, the last element is the log standard deviation\n",
        "    @param num_samples:     int: number of samples to create\n",
        "    @return x_q:            [B, N, K]: samples from q distribution, where N is the number of samples\n",
        "    \"\"\"\n",
        "    # Init\n",
        "    b, k_ = params_q.shape\n",
        "    k = k_ - 1\n",
        "    mu_q, log_sig_q = params_q[:, :-1], params_q[:, -1]  # [B, K], [B]\n",
        "\n",
        "    x_q = torch.distributions.normal.Normal(mu_q, log_sig_q[:, None].exp()).sample([num_samples])  # [N, B, K]\n",
        "    x_q = x_q.permute(1, 0, 2)  # [B, N, K]\n",
        "    return x_q\n",
        "\n",
        "\n",
        "def unit_test_rsample(func_rsample=rsample, func_gt_rsample=gt_rsample):\n",
        "    \"\"\"\n",
        "    Unit test helper. Do not modify.\n",
        "    \"\"\"\n",
        "    b, k = 1, 2\n",
        "    mu_q = torch.randn([b, k]) * 10.0\n",
        "    log_sigma_q = torch.rand([b])\n",
        "\n",
        "    params_q = torch.cat([mu_q, log_sigma_q[:, None]], dim=-1)\n",
        "\n",
        "    num_samples = 100000\n",
        "\n",
        "    compute_x_q = func_rsample(params_q, num_samples)            # [B, N, K]\n",
        "    true_x_q = func_gt_rsample(params_q, num_samples)            # [B, N, K]\n",
        "\n",
        "    mean_error = (compute_x_q.mean(dim=1) - true_x_q.mean(dim=1)).mean(dim=-1)  # [B]\n",
        "    std_error = (compute_x_q.std(dim=1, unbiased=False) - true_x_q.std(dim=1, unbiased=False)).mean(dim=-1)  # [B]\n",
        "    error = mean_error.abs().mean().item() + std_error.abs().mean().item()\n",
        "    if error < 1e-2:\n",
        "        print(\"Your implementation is likely correct.\")\n",
        "    else:\n",
        "        print(\"Your implementation is likely incorrect.\")\n",
        "\n",
        "    # visualize the distribution\n",
        "    figs, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axs[0].hist(compute_x_q[0, :, 0].numpy(), bins=100, alpha=0.5, label='compute')\n",
        "    axs[0].hist(true_x_q[0, :, 0].numpy(), bins=100, alpha=0.5, label='true')\n",
        "    axs[0].legend()\n",
        "    axs[0].set_title(\"Distribution of samples (1st dimension)\")\n",
        "\n",
        "    axs[1].hist(compute_x_q[0, :, 1].numpy(), bins=100, alpha=0.5, label='compute')\n",
        "    axs[1].hist(true_x_q[0, :, 1].numpy(), bins=100, alpha=0.5, label='true')\n",
        "    axs[1].legend()\n",
        "    axs[1].set_title(\"Distribution of samples (2nd dimension)\")\n",
        "\n",
        "\n",
        "set_seed(seed=SEED)\n",
        "unit_test_rsample()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whl7MPeJnOKw"
      },
      "source": [
        "## Q1.5 [5 Pts]: Implement ELBO loss\n",
        "\n",
        "Recall that the ELBO loss reads as follows:\n",
        "\n",
        "$$\\mathcal{L}_{\\text{ELBO}} = -\\mathbb{E}_{q(\\mathbf{z} \\mid \\mathbf{x})} \\left[\\log p(\\mathbf{x} \\mid \\mathbf{z}) \\right] + \\text{KL}\\left(q(\\mathbf{z} \\mid \\mathbf{x}) \\| p(\\mathbf{z}) \\right),$$\n",
        "\n",
        "where $\\mathbf{x}$ is the observable data, $\\mathbf{z}$ is the latent variable, and $q$ and $p$ are the encoder and decoder respectively.\n",
        "\n",
        "To compute the full ELBO loss for a simple case where the encoder $q(\\mathbf{z} \\mid \\mathbf{x})$ and decoder $p(\\mathbf{x} \\mid \\mathbf{z})$ are parameterized by Gaussians with known parameters, and $p(\\mathbf{z})$ is the standard normal distribution, we can follow these steps:\n",
        "\n",
        "* Draw $N$ samples from $q(\\mathbf{z} \\mid \\mathbf{x})$ using the reparameterization trick, where $N$ is the number of samples.\n",
        "\n",
        "* Pass the samples through the decoder $p(\\mathbf{x} \\mid \\mathbf{z})$ to obtain reconstructions.\n",
        "\n",
        "* Compute the log-likelihood of the reconstructions under $p(\\mathbf{x} \\mid \\mathbf{z})$.\n",
        "\n",
        "* Average the log-likelihood over the $N$ samples to obtain an unbiased estimate of $-\\mathbb{E}_{q(\\mathbf{z} \\mid \\mathbf{x})} \\left[\\log p(\\mathbf{x} \\mid \\mathbf{z}) \\right]$.\n",
        "\n",
        "* Compute the exact KL divergence $\\text{KL}\\left(q(\\mathbf{z} \\mid \\mathbf{x}) \\| p(\\mathbf{z}) \\right)$ to obtain the full ELBO Loss.\n",
        "\n",
        "**Task**: given $q(\\mathbf{z} \\mid \\mathbf{x})$ and $p(\\mathbf{x} \\mid \\mathbf{z})$ parameterized by Gaussians, compute the full ELBO loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKFL6f_jnOKw"
      },
      "outputs": [],
      "source": [
        "def elbo(x: torch.Tensor, params_q: torch.Tensor, num_samples: int, func_decoder, log_sig_x: float) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    ### Fill in this function ###\n",
        "    Obtain samples from Gaussian distributions, provided the mean and covariance parameters.\n",
        "    @param x:               [B, D]: observable data\n",
        "    @param params_q:        [B, K+1]: parameters of B many q distribution, the first K elements are the mean, the last element is the log standard deviation\n",
        "    @param num_samples:     int: N, number of samples to create\n",
        "    @param func_decoder:    function: decoder function, takes in [B, N, K] and returns [B, N, D]\n",
        "    @return elbo:           [B]: elbo for each data point\n",
        "    \"\"\"\n",
        "    # note: to pass through decoder, one can use mu_x = func_decoder(z_q)  # mu_x: [B, N, D] <- z_q: [B, N, K]\n",
        "    # note: the gt_rsample function is provided for you for unit testing\n",
        "    z_q = gt_rsample(params_q, num_samples)  # [B, N, K], z samples\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return elbo\n",
        "\n",
        "\n",
        "def gt_elbo(x: torch.Tensor, params_q: torch.Tensor, num_samples: int, func_decoder, log_sig_x: float) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Ground truth ELBO function. Do not modify.\n",
        "    @param x:               [B, D]: observable data\n",
        "    @param params_q:        [B, K+1]: parameters of B many q distribution, the first K elements are the mean, the last element is the log standard deviation\n",
        "    @param num_samples:     int: N, number of samples to create\n",
        "    @param func_decoder:    function: decoder function, takes in [B, N, K] and returns [B, N, D]\n",
        "    @return elbo:           [B]: elbo for each data point\n",
        "    \"\"\"\n",
        "    z_q = gt_rsample(params_q, num_samples)  # [B, N, K], z samples\n",
        "    mu_x = func_decoder(z_q)  # [B, N, D] <- [B, N, K]\n",
        "\n",
        "    log_sig_x = torch.tensor(log_sig_x, dtype=torch.float32, device=x.device)  # [1]\n",
        "    log_sig_x = log_sig_x.view(1, 1).expand(mu_x.shape[0], mu_x.shape[1])  # [B, N]\n",
        "    log_prob_x = gt_log_prob(x[:, None, :].expand(-1, num_samples, -1), mu_x, log_sig_x).mean(dim=-1)  # [B] <- [B, N]\n",
        "\n",
        "    params_std_norm = torch.zeros_like(params_q)  # [B, K+1]\n",
        "    kl_q_p = gt_kl_q_p_exact(params_q, params_std_norm)  # [B]\n",
        "    elbo = -log_prob_x + kl_q_p  # [B]\n",
        "\n",
        "    return elbo\n",
        "\n",
        "def unit_test_elbo(func_elbo=elbo, func_gt_elbo=gt_elbo):\n",
        "    \"\"\"\n",
        "    Unit test helper. Do not modify.\n",
        "    \"\"\"\n",
        "    b, k, n, d = 10, 1, 1, 64\n",
        "    mu_q = torch.randn([b, k])\n",
        "    log_sigma_q = torch.rand([b])\n",
        "\n",
        "    x = torch.randn([b, d])  # [B, N, D]\n",
        "    params_q = torch.cat([mu_q, log_sigma_q[:, None]], dim=-1)  # [B, K+1]\n",
        "\n",
        "    # func_decoder = lambda x: x * 0.5 + 1.0  # [B, N, K] -> [B, N, D]\n",
        "    func_decoder = nn.Linear(k, d, bias=True)\n",
        "    log_sig_x = torch.rand([1]).item()\n",
        "\n",
        "    set_seed(seed=SEED)\n",
        "    comp_elbo = func_elbo(x, params_q, n, func_decoder, log_sig_x)\n",
        "\n",
        "    set_seed(seed=SEED)\n",
        "    true_elbo = func_gt_elbo(x, params_q, n, func_decoder, log_sig_x)\n",
        "\n",
        "    error = torch.abs(comp_elbo - true_elbo).mean()\n",
        "    if error < 1e-5:\n",
        "        print(\"Your implementation is correct.\")\n",
        "    else:\n",
        "        print(\"Your implementation is incorrect.\")\n",
        "\n",
        "\n",
        "set_seed(seed=SEED)\n",
        "unit_test_elbo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDd0uUv8nOKw"
      },
      "source": [
        "## Q2.1 [10 Pts]: Build and train the VAE model\n",
        "\n",
        "Now that we have implemented the necessary functions in Q1, we can use them to build a simple VAE and begin training.\n",
        "\n",
        "**Task**: in this part, you are given the full code for the VAE model.\n",
        "We need to re-use and properly adapt the previously implemented `rsample`, `log_prob`, and `kl_q_p_exact` functions for training purposes.\n",
        "If your functions passed the previous unit tests, they should be ready to use.\n",
        "\n",
        "You should be able to observe a rising trend in the ELBO training curve. We require you to implement efficient training that completes within 5 minutes on a GPU (e.g., a T4 available in Google Colab); otherwise, you will lose points.\n",
        "\n",
        "Note: if you can't pass Q2.1, you can still proceed to Q2.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnusKXWDnOKw"
      },
      "outputs": [],
      "source": [
        "class SimpleVAE(nn.Module):\n",
        "  \"\"\"\n",
        "  Simple VAE with convolutional encoder and decoder.\n",
        "  \"\"\"\n",
        "  def __init__(self, K=2, num_filters=32):\n",
        "    \"\"\"\n",
        "    We aim to build a simple VAE with the following architecture for the MNIST dataset.\n",
        "    @param K: int: Bottleneck dimensionality\n",
        "    @param num_filters: int: Number of filters [default: 32]\n",
        "    \"\"\"\n",
        "\n",
        "    super(SimpleVAE, self).__init__()\n",
        "\n",
        "    self.kernel_size = 5  # fixed\n",
        "    self.image_size = 28  # fixed\n",
        "    self.size_after_conv = self.image_size - 2 * 2 * (self.kernel_size // 2)\n",
        "    self.flat_size_after_conv = self.size_after_conv * self.size_after_conv * num_filters\n",
        "\n",
        "    # encoder\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Conv2d(1, num_filters, self.kernel_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(num_filters, num_filters, self.kernel_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(self.flat_size_after_conv, K+1)\n",
        "    )\n",
        "\n",
        "    # decoder\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.Linear(K, self.flat_size_after_conv),\n",
        "        nn.Unflatten(1, (num_filters, self.size_after_conv, self.size_after_conv)),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(num_filters, num_filters, self.kernel_size),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(num_filters, 1, self.kernel_size),\n",
        "    )\n",
        "\n",
        "    # decoder variance parameter: for simplicity, we define a scalar log_sig_x for all pixels\n",
        "    self.log_sig_x = nn.Parameter(torch.zeros(()))\n",
        "\n",
        "  def decode(self, samples_z):\n",
        "    \"\"\"\n",
        "    Wrapper for decoder\n",
        "    @param samples_z:   [B, N, K]: samples from the latent space, B: batch size, N: number of samples, K: latent dimensionality\n",
        "    @return mu_xs:      [B, N, C, H, W]: mean of the reconstructed data, B: batch size, N: number of samples, D: data dimensionality\n",
        "    \"\"\"\n",
        "    b, n, k = samples_z.shape\n",
        "    s_z = samples_z.view(b * n, -1)                                     # [B*N, K]\n",
        "    s_z = self.decoder(s_z)                                             # [B*N, D]\n",
        "    mu_xs = s_z.view(b, n, 1, self.image_size, self.image_size)         # [B, N, C, H, W]\n",
        "    return mu_xs\n",
        "\n",
        "  def elbo(self, x, n=1):\n",
        "    \"\"\"\n",
        "    Run input end to end through the VAE and compute the ELBO using n samples of z\n",
        "    @param x:       [B, C, H, W]: input image, B: batch size, C: number of channels, H: height, W: width\n",
        "    @param n:       int: number of samples of z sample and reconstruction samples\n",
        "    @return elbo:   scalar: aggregated ELBO loss for each image in the batch\n",
        "    \"\"\"\n",
        "    phi = self.encoder(x)     # [B, K+1] <- [B, C, H, W]\n",
        "    zs = rsample(phi, n)      # [B, N, K] <- [B, K+1]\n",
        "    mu_xs = self.decode(zs)   # [B, N, C, H, W] <- [B, N, K]\n",
        "\n",
        "    b, c, h, w = x.shape\n",
        "    x_flat = x.view(b, 1, -1).expand(-1, n, -1)                 # [B, N, C*H*W] <- [B, 1, C*H*W] <- [B, C, H, W]\n",
        "    mu_xs_flat = mu_xs.view(b, n, -1)                           # [B, N, C*H*W]\n",
        "    log_sig_x = self.log_sig_x.view(1, 1).expand(x.size(0), n)  # [B, N]\n",
        "\n",
        "    # note: we use the exact KL divergence here, but we could also use the Monte Carlo approximation\n",
        "    # note: we didn't use the ELBO loss implemented in Q1.5, because it is less numerically stable\n",
        "    elbo_loss = log_prob(x_flat, mu_xs_flat, log_sig_x).mean() - kl_q_p_exact(phi, torch.zeros_like(phi)).mean()\n",
        "    return elbo_loss\n",
        "\n",
        "\n",
        "K_VAE = 2\n",
        "vae = SimpleVAE(K=K_VAE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQYNSkqPnOKw"
      },
      "outputs": [],
      "source": [
        "def train_vae(model, dataset, epochs=10, n_samples=1000, batch_size=128):\n",
        "  \"\"\"\n",
        "  VAE trainer.\n",
        "  \"\"\"\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0)\n",
        "  model.to(DEVICE)\n",
        "  model.train()\n",
        "  elbo_vals = []\n",
        "  loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "  for epoch in trange(epochs, desc='Epochs'):\n",
        "    pbar = tqdm(loader, total=len(dataset) // batch_size, desc='Batches', leave=False)\n",
        "    for im, _ in pbar:\n",
        "      im = im.to(DEVICE)\n",
        "\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "      loss = -model.elbo(im, n=n_samples)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      elbo_vals.append(-loss.item())\n",
        "\n",
        "      pbar.set_description('Batches avg ELBO: {:.2f}'.format(np.mean(elbo_vals)))\n",
        "  model.to('cpu')\n",
        "  model.eval()\n",
        "  return elbo_vals\n",
        "\n",
        "\n",
        "# Train the VAE\n",
        "elbo_vals = train_vae(vae, train_set, epochs=20, n_samples=10, batch_size=256)\n",
        "print(\"Learned sigma_x is {}\".format(torch.exp(vae.log_sig_x)))\n",
        "\n",
        "# Plot ELBO training curve\n",
        "plt.figure()\n",
        "plt.plot(elbo_vals)\n",
        "plt.xlabel('Batch #')\n",
        "plt.ylabel('ELBO')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CajclNVnOKw"
      },
      "source": [
        "## Q2.2 [5 Pts]: Generate new samples using trained VAE model\n",
        "Now we can generate new samples using the VAE model that we trained. Recall that the decoder is responsible for generating reconstructions:\n",
        "\n",
        "$$\\mathbf{z} \\overset{\\text{VAE}}{\\longrightarrow} p_{\\mathbf{w_d}}(\\mathbf{x}|\\mathbf{z})$$\n",
        "\n",
        "During training, $\\mathbf{z} \\sim q(\\mathbf{z} \\mid \\mathbf{x})$ is sampled from a distribution given by the encoder. The encoder takes observable data $\\mathbf{x}$ as input.\n",
        "\n",
        "During inference, we no longer have access to $\\mathbf{x}$, and instead we sample $\\mathbf{z} \\sim q(\\mathbf{z})$ from its prior distribution, which is standard normal.\n",
        "\n",
        "To generate new images, we pass the $\\mathbf{z}$ samples to the decoder $p(\\mathbf{x}|\\mathbf{z})$ and obtain the mean of the reconstruction, which serves as our samples.\n",
        "\n",
        "**Task**: use the VAE model's `decode` method to create new samples, given a pretrained model's weight.\n",
        "\n",
        "Note: you don't need functions from Q1 or Q2.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGS-Mhk8nOKx"
      },
      "outputs": [],
      "source": [
        "# download vae unit test file\n",
        "!gdown 1bRN-alJ8Ayidu3tprf876FhAcDjGa70a -q -O cpen455_24w2_vae_unit_test.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rHQGwcjnOKx"
      },
      "outputs": [],
      "source": [
        "def generate_images(vae, K=2, n_images=9):\n",
        "  \"\"\"\n",
        "  ### Fill in this function ###\n",
        "  Generate new images from the given VAE. Sample z from a unit gaussian, pass through autoencoder.decode()\n",
        "  @param vae:       VAE model\n",
        "  @param K:         int: latent dimensionality\n",
        "  @param n_images:  int: number of images to generate\n",
        "  \"\"\"\n",
        "  # Init\n",
        "  output_shape = (n_images,) + data_shape   # [X, C, H, W] = [X, 1, 28, 28] for MNIST\n",
        "  samples_z = torch.randn(n_images, 1, K)   # [X, 1, K]\n",
        "\n",
        "  with torch.no_grad():\n",
        "    ### START CODE HERE ###\n",
        "    ### END CODE HERE ###\n",
        "    return samples_x\n",
        "\n",
        "\n",
        "def unit_test_generate_images(vae=vae):\n",
        "  vae_unit_test_data = torch.load('cpen455_24w2_vae_unit_test.pt')\n",
        "  vae.load_state_dict(vae_unit_test_data['model'])\n",
        "  vae.eval()\n",
        "  vae = vae.to('cpu')\n",
        "  img_samples = vae_unit_test_data['samples']\n",
        "  images = generate_images(vae, K_VAE, n_images=9)\n",
        "\n",
        "  error = torch.abs(images - img_samples).mean()\n",
        "  if error < 1e-5:\n",
        "    print(\"Your implementation is correct.\")\n",
        "  else:\n",
        "    print(\"Your implementation is incorrect.\")\n",
        "\n",
        "  print(\"See below visualizations as a sanity check. (The images should look exactly the same.)\")\n",
        "\n",
        "  plot_images(images, plt_title='Images Generated from VAE (your implementataion)')\n",
        "  plot_images(img_samples, plt_title='Images Generated from VAE (true implementation)')\n",
        "\n",
        "\n",
        "set_seed(seed=SEED)\n",
        "unit_test_generate_images()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}