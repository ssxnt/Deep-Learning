{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AS6zKmYSwVD"
   },
   "source": [
    "# Programming Assignment 1: Convolution and Back-Propagation\n",
    "\n",
    "**UBC CPEN 455: Deep Learning, 2024 Winter Term 2**\n",
    "\n",
    "**Created By Renjie Liao**\n",
    "\n",
    "**Date: Feb. 19, 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhakGhuGTBzQ"
   },
   "source": [
    "---\n",
    "# Setup\n",
    "\n",
    "We will use PyTorch to implement this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PoqjO9gIKMMV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sant/anaconda3/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pdb\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from math import pi\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "## load MNIST images\n",
    "B = 5 # batch size\n",
    "train_set = datasets.MNIST('./data',\n",
    "                            train=True,\n",
    "                            download=True,\n",
    "                            transform=transforms.ToTensor())\n",
    "\n",
    "loader = torch.utils.data.DataLoader(train_set, batch_size=B)\n",
    "\n",
    "## load a batch of MNIST images as a PyTorch tensor (shape: B x C x H x W)\n",
    "# B: batch size\n",
    "# C: number of channels\n",
    "# H: height of images\n",
    "# W: width of images\n",
    "img, label = next(iter(loader)) # img shape: B x C x H x W, label shape: B X 1\n",
    "\n",
    "## create a random filter (shape: D x C x K x K)\n",
    "K = 3 # kernel size\n",
    "P = 1 # padding size\n",
    "C = 1 # channel size\n",
    "D = 2 # number of filters\n",
    "filter = torch.randn(D, C, K, K) # filter shape: D x C x K x K\n",
    "#print(filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6t6YO5QlYhbX"
   },
   "source": [
    "---\n",
    "# Q1 [60Pts]: 2D convolution and its gradient\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koMGWjFvWWOP"
   },
   "source": [
    "## 1.1 [30Pts]  Implement 2D convolution:\n",
    "\n",
    "Discrete convolution can be implemented in multiple ways, e.g., matrix multiplication in spatial/Fourier domains.\n",
    "\n",
    "First, let us take a look at 1D convolution in spatial domain. Suppose we have a 1D signal with $n$ elements $x_1, x_2, \\dots, x_n$ and a 1D filter with $m$ weights $h_1, h_2, \\dots, h_m$. Note that we typically use filters with odd sizes for the ease of indexing.\n",
    "\n",
    "The (discrete) convolution with zero-padding and stride 1 is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "    y = h \\ast x = \\sum_{i=1}^{n} \\sum_{j=1}^{m} h_j x_{i - \\lfloor m/2 \\rfloor - 1 + j},\n",
    "\\end{align}\n",
    "where padded values $x_{-\\lfloor m/2 \\rfloor + 1}, \\dots, x_{0}, x_{n+1}, \\dots, x_{n - \\lfloor m/2 \\rfloor - 1 + m}$ are all zeros.\n",
    "\n",
    "If you forget about the concepts of padding and stride, take a look at [this guide](https://arxiv.org/pdf/1603.07285.pdf) or [these pictures](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md).\n",
    "\n",
    "In the 1D case, we can illustrate the two matrix multiplication views of spatial convolution as below.\n",
    "\n",
    "1.   **im2col**:\n",
    "The key idea is to first extract the spatial windows from the signal $x$ for individual convolutions and then perform convolutions (i.e., dot product with the filter).\n",
    "If we put each window as a column in a matrix (the right one in RHS below), then we can perform convolution via the following matrix multiplication (N.B.: the products between the filter and individual columns can be done in parallel).\n",
    "\n",
    "\\begin{align}\n",
    "    y^\\top = (h \\ast x)^{\\top} = \\begin{bmatrix}\n",
    "                h_m & h_{m-1} & \\cdots & h_3 & h_2 & h_1\n",
    "            \\end{bmatrix}\n",
    "            \\begin{bmatrix}\n",
    "                x_{m - \\lfloor m/2 \\rfloor} & x_{m - \\lfloor m/2 \\rfloor + 1} & \\cdots & x_m & x_{m+1} & \\cdots & 0 & 0 \\\\\n",
    "                \\vdots & \\vdots & \\cdots & x_{m-1} & x_m & \\cdots & \\vdots & \\vdots \\\\\n",
    "                x_1 & x_2 & \\cdots & \\vdots & x_{m-1} & \\cdots  & x_n & 0 \\\\\n",
    "                0 & x_1 & \\cdots & \\vdots & \\vdots & \\cdots  & x_{n-1} & x_n \\\\\n",
    "                \\vdots & 0 & \\cdots & \\vdots & \\vdots & \\cdots & \\vdots & \\vdots \\\\                        \n",
    "                0 & 0 & \\cdots & x_1 & x_2 & \\cdots & x_{n - \\lfloor m/2 \\rfloor+1} & x_{n - \\lfloor m/2 \\rfloor}\n",
    "            \\end{bmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "2.   **filter2row**: The key idea is to convert the filter and the signal to a sparse cyclic matrix and a vector respectively.\n",
    "Then the convolution is simply the matrix multiplication between the filter and the signal.\n",
    "\n",
    "\\begin{align}\n",
    "        y = h \\ast x =\n",
    "            \\begin{bmatrix}\n",
    "                h_{\\lfloor m/2 \\rfloor + 1} & h_{\\lfloor m/2 \\rfloor + 2} & \\cdots & h_m & 0 & \\cdots & \\cdots & \\cdots & \\cdots & 0 \\\\\n",
    "                h_{\\lfloor m/2 \\rfloor} & h_{\\lfloor m/2 \\rfloor + 1} & \\cdots & h_{m-1} & h_m & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\\n",
    "                \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "                h_1 & h_2 & \\cdots & \\cdots & \\cdots & \\cdots & h_m & 0 & \\cdots & 0 \\\\\n",
    "                0 & h_1 & h_2 & \\cdots & \\cdots & \\cdots & \\cdots & h_m & \\cdots & 0 \\\\\n",
    "                \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "                0 & 0 & \\cdots & \\cdots & \\cdots & 0 & h_1 & h_2 & \\cdots & h_{\\lfloor m/2 \\rfloor + 2} \\\\                \n",
    "                0 & 0 & \\cdots & \\cdots & & \\cdots \\cdots & 0 & h_1 & \\cdots & h_{\\lfloor m/2 \\rfloor + 1}\n",
    "            \\end{bmatrix}\n",
    "            \\begin{bmatrix}\n",
    "                x_1 \\\\\n",
    "                x_2 \\\\\n",
    "                x_3 \\\\\n",
    "                \\vdots \\\\\n",
    "                x_n\n",
    "            \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "**Task**:\n",
    "Implement the 2D convolution in the spatial domain via matrix multiplication following the above two views: **im2col** and **filter2row**.\n",
    "The starter code is provided below.\n",
    "You just need to fill in the missing parts of function ***conv2d_im2col*** and ***conv2d_filter2row***.\n",
    "If your implementation is correct, the ***unit_test*** will output:\n",
    "\n",
    "*Your implementation of xxx is correct!*\n",
    "\n",
    "Otherwise, it will output:\n",
    "\n",
    "*Your implementation of xxx is wrong!*\n",
    "\n",
    "**N.B.**: we assume the strides along height and width are the same and the kernel is square\n",
    "\n",
    "**Hint**: you can reduce the 2D case to 1D and follow the above construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-IRaDddEdgT0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation of conv2d_im2col is correct!\n",
      "Your implementation of conv2d_filter2row is correct!\n"
     ]
    }
   ],
   "source": [
    "## implement the following two functions\n",
    "def conv2d_im2col(img, filter, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
    "    ### Fill in this function ###\n",
    "    # Args:\n",
    "    #   img: images, shape B x C x H x W\n",
    "    #   filter: filters, shape D x C x K x K\n",
    "    #   channel_size: number of channels, scalar (C)\n",
    "    #   num_filters: number of filters, scalar (D)\n",
    "    #   kernel_size: kernel size, scalar (K)\n",
    "    #   stride: stride size, scalar\n",
    "    #   padding: padding size, scalar\n",
    "    #\n",
    "    # Returns:\n",
    "    #   out: convoluted images, shape B x D x H x W\n",
    "    \n",
    "    y_h = int((img.shape[2]+2*padding-filter.shape[2])/stride+1)\n",
    "    y_w = int((img.shape[3]+2*padding-filter.shape[2])/stride+1)\n",
    "    #print(y_h)\n",
    "    #print(y_w)\n",
    "    #print(img)\n",
    "    #print(img.size())\n",
    "    #print(filter.size())\n",
    "    #print(filter)\n",
    "    \n",
    "    img_padded = pad(img, padding)\n",
    "    #print(img_padded)\n",
    "    \n",
    "    samples = []\n",
    "    for b in range(img.shape[0]):\n",
    "        for i in range(y_h):\n",
    "            for j in range(y_w):\n",
    "                #sample = img_padded[b,0:img.shape[1],i*stride:i*stride+kernel_size,j*stride:j*stride+kernel_size]\n",
    "                sample = get_sample(img_padded, b, img.shape[1], i, j, stride, kernel_size)\n",
    "                samples.append(sample.flatten())\n",
    "    \n",
    "    samples = torch.stack(samples)\n",
    "    samples = samples.reshape(img.shape[0],y_h*y_w,-1)\n",
    "    #print(samples.size())\n",
    "    samples = samples.permute(0, 2, 1)\n",
    "    #print(samples.size())\n",
    "    #print(filter.size())\n",
    "    filter = filter.reshape(num_filters,-1)\n",
    "    #print(filter.size())\n",
    "    \n",
    "    out = []\n",
    "    for i in range(img.shape[0]):\n",
    "        z = filter@samples[i]\n",
    "        out.append(z.reshape(num_filters,y_h,y_w))\n",
    "    \n",
    "    out = torch.stack(out)\n",
    "    #print(out.size())\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def conv2d_filter2row(img, filter, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
    "    ### Fill in this function ###\n",
    "    # Args:\n",
    "    #   img: images, shape B x C x H x W\n",
    "    #   filter: filters, shape D x C x K x K\n",
    "    #   channel_size: number of channels, scalar (C)\n",
    "    #   num_filters: number of filters, scalar (D)\n",
    "    #   kernel_size: kernel size, scalar (K)\n",
    "    #   stride: stride size, scalar\n",
    "    #   padding: padding size, scalar\n",
    "    #\n",
    "    # Returns:\n",
    "    #   out: convoluted images, shape B x D x H x W\n",
    "\n",
    "    y_h = int((img.shape[2]+2*padding-filter.shape[2])/stride+1)\n",
    "    y_w = int((img.shape[3]+2*padding-filter.shape[2])/stride+1)\n",
    "\n",
    "    image_padded = pad(img, padding)\n",
    "    m = torch.zeros((num_filters, img.shape[1]*filter.shape[2]*filter.shape[3]))\n",
    "    #print(m.shape)\n",
    "    \n",
    "    for d in range(num_filters):\n",
    "        i = 0\n",
    "        for c in range(img.shape[1]):\n",
    "            for k1 in range(filter.shape[2]):\n",
    "                for k2 in range(filter.shape[3]):\n",
    "                    m[d,i] = filter[d,c,k1,k2]\n",
    "                    i += 1\n",
    "                    \n",
    "    out = torch.zeros((img.shape[0],num_filters,y_h,y_w))\n",
    "    \n",
    "    for b in range(img.shape[0]):\n",
    "        samples = torch.zeros((img.shape[1]*filter.shape[2]*filter.shape[3],y_h*y_w))\n",
    "        j = 0\n",
    "        for h in range(y_h):\n",
    "            for w in range(y_w):\n",
    "                sample = get_sample(image_padded,b,img.shape[1],h,w,stride,filter.shape[2])\n",
    "                #print(sample.shape)\n",
    "                samples[:,j] = sample.reshape(-1)\n",
    "                #print(samples.shape)\n",
    "                j += 1\n",
    "                \n",
    "        z = m@samples\n",
    "        #print(z.shape)\n",
    "        out[b] = z.view(num_filters, y_h, y_w)\n",
    "       \n",
    "    #print(out.shape)\n",
    "        \n",
    "    return out\n",
    "\n",
    "    \n",
    "def pad(img, padding):\n",
    "    # pads the original image and returns the padded image\n",
    "    \n",
    "    if padding != 0:\n",
    "        img_padded = torch.zeros((img.shape[0],img.shape[1],img.shape[2]+2*padding,img.shape[3]+2*padding))\n",
    "        #print(img_padded.size())\n",
    "        for i in range(img.shape[0]):\n",
    "            for j in range(img.shape[1]):\n",
    "                for k in range(img.shape[2]):\n",
    "                    for l in range(img.shape[3]):\n",
    "                        img_padded[i,j,k+padding,l+padding] = img[i,j,k,l]\n",
    "    else:\n",
    "        img_padded = img \n",
    "        \n",
    "    return img_padded\n",
    "\n",
    "\n",
    "def get_sample(p_img, b, c, h, w, s, k, per_c=False):\n",
    "    # calculates and obtains sample within a fixed range during 2d convolution and returns the sample\n",
    "    \n",
    "    if not per_c:\n",
    "        s = p_img[b,0:c,h*s:h*s+k,w*s:w*s+k]\n",
    "    else:\n",
    "        s = p_img[b,c,h*s:h*s+k,w*s:w*s+k]\n",
    "        \n",
    "    return s\n",
    "\n",
    "\n",
    "def unit_test_conv2d(img, filter, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
    "    # call your implemented \"im2col\" conv2D\n",
    "    y_im2col = conv2d_im2col(img, filter, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    # ground truth conv2D\n",
    "    y_gt = F.conv2d(img, filter, stride=stride, padding=padding)\n",
    "\n",
    "    diff = (y_im2col - y_gt).norm()\n",
    "    if diff < 1.0e-5:\n",
    "        print(\"Your implementation of conv2d_im2col is correct!\")\n",
    "    else:\n",
    "        print(\"Your implementation of conv2d_im2col is wrong!\")\n",
    "\n",
    "    # call your implemented \"im2col\" conv2D\n",
    "    y_filter2row = conv2d_filter2row(img, filter, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    diff = (y_filter2row - y_gt).norm()\n",
    "    if diff < 1.0e-5:\n",
    "        print(\"Your implementation of conv2d_filter2row is correct!\")\n",
    "    else:\n",
    "        print(\"Your implementation of conv2d_filter2row is wrong!\")\n",
    "\n",
    "\n",
    "unit_test_conv2d(img, filter, channel_size=C, num_filters=D, kernel_size=K, stride=1, padding=P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0l4zm6mEXCxO"
   },
   "source": [
    "## 1.2 [20Pts] Implement the gradient of 2D convolution\n",
    "\n",
    "We now turn to the gradient of 2D convolution.\n",
    "In particular, given a batch of images $x$ with the shape $B \\times C \\times H \\times W$ and filters $h$ with shape $D \\times C \\times K \\times K$, we can view the convolution (zero-padding and stride 1) as a function\n",
    "\\begin{align}\n",
    "    y = f(h, x),\n",
    "\\end{align}\n",
    "that would produce an output tensor $y$ with shape $B \\times D \\times H \\times W$.\n",
    "\n",
    "If we vectorize $x$, $h$, and $y$, then the Jacobian matrix $∇f = [\\frac{\\partial y}{\\partial h}, \\frac{\\partial y}{\\partial x}]$ would be of shape $BDHW \\times (DCKK + BCHW)$.\n",
    "In practice, we almost never need to compute the Jacobian matrix directly as it is unnecessary for back-propagation.\n",
    "Instead, we often need to compute the product between the transposed Jacobian and a vector (a.k.a. vector-Jacobian product), i.e., ${\\frac{\\partial y}{\\partial h}}^{\\top} v$ and ${\\frac{\\partial y}{\\partial x}}^{\\top} v$.\n",
    "For example, the vector $v$ could the gradient of some loss $\\ell$ (scalar) w.r.t. the output above (i.e., $\\frac{\\partial \\ell}{\\partial y}$).\n",
    "\n",
    "**Task**:\n",
    "Given input images $x$, filters $h$, output $y$, and a vector $v$, implement the gradients ${\\frac{\\partial y}{\\partial h}}^{\\top} v$ and ${\\frac{\\partial y}{\\partial x}}^{\\top} v$ in the function below.\n",
    "\n",
    "**N.B.**: The function needs to return the gradients in the original shapes, i.e., ${\\frac{\\partial y}{\\partial h}}^{\\top} v$ should have the same shape as $h$ ($D \\times C \\times K \\times K$) and ${\\frac{\\partial y}{\\partial x}}^{\\top} v$ should have the same shape as $x$ ($B \\times C \\times H \\times W$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Vtz-vk95d9WR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation of grad_img is correct!\n",
      "Your implementation of grad_filter is correct!\n"
     ]
    }
   ],
   "source": [
    "## implement the following functions\n",
    "def grad_conv2d(img, filter, out, grad_out, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
    "    ### Fill in this function ###\n",
    "    # Args:\n",
    "    #   img: images, shape B x C x H x W\n",
    "    #   filter: filters, shape D x C x K x K\n",
    "    #   out: convoluted images, shape B x D x H x W\n",
    "    #   grad_out: gradient w.r.t. output, shape B x D x H x W\n",
    "    #   channel_size: number of channels, scalar (C)\n",
    "    #   num_filters: number of filters, scalar (D)\n",
    "    #   kernel_size: kernel size, scalar (K)\n",
    "    #   stride: stride size, scalar\n",
    "    #   padding: padding size, scalar\n",
    "    #\n",
    "    # Returns:\n",
    "    #   grad_img: gradient w.r.t. img, shape B x C x H x W\n",
    "    #   grad_filer: gradient w.r.t. filter, shape D x C x K x K\n",
    "    \n",
    "    #print(grad_out)\n",
    "    #print(grad_out.shape)\n",
    "    padded_image = pad(img, padding)\n",
    "    padded_gradient_image = torch.zeros_like(padded_image)\n",
    "    grad_filter = torch.zeros(filter.shape[0],filter.shape[1],filter.shape[2],filter.shape[3])\n",
    "    grad_img = torch.zeros(img.shape[0],img.shape[1],img.shape[2],img.shape[3])\n",
    "    \n",
    "    for b in range(img.shape[0]):\n",
    "        for d in range(filter.shape[0]):\n",
    "            for c in range(img.shape[1]):\n",
    "                for h in range(img.shape[2]):\n",
    "                    for w in range(img.shape[3]):\n",
    "                        sample = get_sample(padded_image, b, c, h, w, stride, kernel_size, per_c=True)\n",
    "                        #print(sample.shape)\n",
    "                        grad_filter[d,c] += sample*grad_out[b,d,h,w]\n",
    "                        #print(grad_filter.shape)\n",
    "                        \n",
    "    for b in range(img.shape[0]):\n",
    "        for c in range(img.shape[1]):\n",
    "            for d in range(filter.shape[0]):\n",
    "                for h in range(img.shape[2]):\n",
    "                    for w in range(img.shape[3]):\n",
    "                        sample = get_sample(padded_gradient_image, b, c, h, w, stride, kernel_size, per_c=True)\n",
    "                        gradient = sample+filter[d,c]*grad_out[b,d,h,w]\n",
    "                        #print(gradient.shape)\n",
    "                        padded_gradient_image[b,c,h*stride:h*stride+kernel_size,w*stride:w*stride+kernel_size] = gradient\n",
    "    \n",
    "    grad_img = unpad(padded_gradient_image, padding)\n",
    "    #print(grad_img.shape)\n",
    "    \n",
    "    return grad_img, grad_filter\n",
    "\n",
    "\n",
    "def unpad(image, pad):\n",
    "    # \"unpads\" an image back to its original dimensions and returns it\n",
    "    \n",
    "    if pad > 0:\n",
    "        unpadded = image[:,:,pad:image.shape[2]-pad,pad:image.shape[3]-pad]\n",
    "    else:\n",
    "        unpadded = image\n",
    "        \n",
    "    return unpadded\n",
    "\n",
    "\n",
    "def unit_test_grad_conv2d(img, filter, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
    "    filter.requires_grad = True\n",
    "    img.requires_grad = True\n",
    "\n",
    "    ### ground truth conv2D\n",
    "    img_out = F.conv2d(img, filter, stride=stride, padding=padding)\n",
    "\n",
    "    # create a random vector v\n",
    "    v = torch.randn_like(img_out)\n",
    "\n",
    "    # call your implemented \"grad_conv2d\" function\n",
    "    grad_img, grad_filter = grad_conv2d(img, filter, img_out, v, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    # compute ground-truth gradients\n",
    "    grad_img_gt = torch.autograd.grad(img_out, img, grad_outputs=v, retain_graph=True)[0]\n",
    "    grad_filter_gt = torch.autograd.grad(img_out, filter, grad_outputs=v, retain_graph=True)[0]\n",
    "    #pdb.set_trace()\n",
    "\n",
    "    diff = (grad_img - grad_img_gt).norm()\n",
    "    if diff < 1.0e-5:\n",
    "        print(\"Your implementation of grad_img is correct!\")\n",
    "    else:\n",
    "        print(\"Your implementation of grad_img is wrong!\")\n",
    "\n",
    "    diff = (grad_filter - grad_filter_gt).norm()\n",
    "    if diff < 1.0e-5:\n",
    "        print(\"Your implementation of grad_filter is correct!\")\n",
    "    else:\n",
    "        print(\"Your implementation of grad_filter is wrong!\")\n",
    "\n",
    "unit_test_grad_conv2d(img, filter, channel_size=C, num_filters=D, kernel_size=K, stride=1, padding=P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QI9J5CDCUSCc"
   },
   "source": [
    "---\n",
    "## 1.3 [10Pts]: Implement gradient checking via the finite difference approximation\n",
    "\n",
    "We verify the correctness of the implementation of gradient operators by calling PyTorch's autograd function.\n",
    "However, PyTorch's autograd function just calls the gradient operators implemented by the PyTorch team.\n",
    "How do they verify the correctness of their implementation?\n",
    "\n",
    "The answer is **finite difference approximation**.\n",
    "Following the setup in 1.2, given a batch of images $x$ with the shape $B \\times C \\times H \\times W$ and filters $h$ with shape $D \\times C \\times K \\times K$, we have the convolution (zero-padding and stride 1)\n",
    "\\begin{align}\n",
    "    y = f(h, x).\n",
    "\\end{align}\n",
    "\n",
    "Again, mentally vectorize $x$ and $y$ would help us understand the math.\n",
    "Given any vector $v$ with the same shape as $y$, we are interested in computing ${\\frac{\\partial y}{\\partial h}}^{\\top} v$ and ${\\frac{\\partial y}{\\partial x}}^{\\top} v$.\n",
    "These two gradients are equivalent to ${\\frac{\\partial \\ell}{\\partial h}}$ and ${\\frac{\\partial \\ell}{\\partial x}}$ where\n",
    "\\begin{align}\n",
    "    \\ell(h, x) = y^{\\top}v = f(h, x)^{\\top} v.\n",
    "\\end{align}\n",
    "Note that here $\\ell$ becomes a scalar.\n",
    "Based on Talyor's theorem, we have\n",
    "\\begin{align}\n",
    "    d^{\\top} \\frac{\\partial \\ell}{\\partial h} = \\lim_{\\epsilon → 0} \\frac{\\ell(h + \\epsilon \\cdot d, x) - \\ell(h - \\epsilon \\cdot d, x)}{2 ϵ},\n",
    "\\end{align}\n",
    "where $d$ could be any direction vector and $ϵ$ is a scalar.\n",
    "For our purpose, we just need to set $d$ to be the unit vector to compute the per-dimension value of $\\frac{\\partial \\ell}{\\partial h}$.\n",
    "Specifically, if we set $d$ as the $i$-th unit vector $e_i$, i.e., $d[i] = 1$ and $d[j] = 0, \\forall j \\neq i$, we can then compute\n",
    "\\begin{align}\n",
    "    \\frac{\\partial \\ell}{\\partial h}[i] &= \\lim_{\\epsilon → 0} \\frac{\\ell(h + \\epsilon \\cdot e_i, x) - \\ell(h - \\epsilon \\cdot e_i, x)}{2 ϵ} \\\\\n",
    "    & ≈ \\frac{\\ell(h + \\epsilon \\cdot e_i, x) - \\ell(h - \\epsilon \\cdot e_i, x)}{2 ϵ}.\n",
    "\\end{align}\n",
    "\n",
    "**Task**: Implement the finite-difference based gradient checker for ${\\frac{\\partial \\ell}{\\partial h}}$ and ${\\frac{\\partial \\ell}{\\partial x}}$.\n",
    "\n",
    "\n",
    "**N.B.**: For efficiency consideration in the unit test, you can use F.conv2d to compute the convolution in your implementation of *grad_checker*. This assignment is to let you understand how to implement finte-difference. But in pratice, if we want to verify our implementation of conv2d, then we should use our conv2d instead of F.conv2d from PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qgeHgGn1JbBn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation of grad_img is correct!\n",
      "Your implementation of grad_filter is correct!\n"
     ]
    }
   ],
   "source": [
    "## implement the following functions\n",
    "def grad_checker(img, filter, conv, grad_out, epsilon=1.0e-5, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
    "    ### Fill in this function ###\n",
    "    # Args:\n",
    "    #   img: images, shape B x C x H x W\n",
    "    #   filter: filters, shape D x C x K x K\n",
    "    #   conv: convolution function\n",
    "    #   out: convoluted images, shape B x D x H x W\n",
    "    #   grad_out: gradient w.r.t. output, shape B x D x H x W\n",
    "    #   channel_size: number of channels, scalar (C)\n",
    "    #   num_filters: number of filters, scalar (D)\n",
    "    #   kernel_size: kernel size, scalar (K)\n",
    "    #   stride: stride size, scalar\n",
    "    #   padding: padding size, scalar\n",
    "    #\n",
    "    # Returns:\n",
    "    #   grad_img: gradient w.r.t. img, shape B x C x H x W\n",
    "    #   grad_filer: gradient w.r.t. filter, shape D x C x K x K\n",
    "    \n",
    "    grad_filter = torch.zeros(filter.shape[0],filter.shape[1],filter.shape[2],filter.shape[3])\n",
    "    grad_img = torch.zeros(img.shape[0],img.shape[1],img.shape[2],img.shape[3])\n",
    "    \n",
    "    for d in range(filter.shape[0]):\n",
    "        for c in range(filter.shape[1]):\n",
    "            for k1 in range(filter.shape[2]):\n",
    "                for k2 in range(filter.shape[3]):\n",
    "                    fp_copy = filter.clone()\n",
    "                    fp = fp_copy.detach()\n",
    "                    fp[d,c,k1,k2] += epsilon\n",
    "                    #print(fp)\n",
    "                    \n",
    "                    fn_copy = filter.clone()\n",
    "                    fn = fn_copy.detach()\n",
    "                    fn[d,c,k1,k2] -= epsilon\n",
    "            \n",
    "                    #op = conv(img,fp,stride=stride,padding=padding)\n",
    "                    #on = conv(img,fn,stride=stride,padding=padding)\n",
    "                    #print(on)\n",
    "                    \n",
    "                    op = conv2d_im2col(img, fp, channel_size, num_filters, kernel_size, stride, padding)\n",
    "                    on = conv2d_im2col(img, fn, channel_size, num_filters, kernel_size, stride, padding)\n",
    "                    #print(on)\n",
    "\n",
    "                    vp = torch.sum(op*grad_out)\n",
    "                    vn = torch.sum(on*grad_out)\n",
    "                    #print(vp)\n",
    "\n",
    "                    grad_filter[d,c,k1,k2] = (vp-vn)/(2*epsilon)\n",
    "                    #print(grad_filter)\n",
    "                    \n",
    "    #print(fp.shape)\n",
    "    #print(on.shape)\n",
    "    #print(vp.shape)\n",
    "    #print(grad_filter.shape)\n",
    "                    \n",
    "    for b in range(img.shape[0]):\n",
    "        for c in range(img.shape[1]):\n",
    "            for h in range(img.shape[2]):\n",
    "                for w in range(img.shape[3]):\n",
    "                    ip_copy = img.clone()\n",
    "                    ip = ip_copy.detach()\n",
    "                    ip[b,c,h,w] += epsilon\n",
    "                    \n",
    "                    in_copy = img.clone()\n",
    "                    i_n = in_copy.detach()\n",
    "                    i_n[b,c,h,w] -= epsilon\n",
    "                    #print(i_n)\n",
    "            \n",
    "                    #op = conv(ip,filter,stride=stride,padding=padding)\n",
    "                    #on = conv(i_n,filter,stride=stride,padding=padding)\n",
    "                    #print(op)\n",
    "                    \n",
    "                    op = conv2d_im2col(ip, filter, channel_size, num_filters, kernel_size, stride, padding)\n",
    "                    on = conv2d_im2col(i_n, filter, channel_size, num_filters, kernel_size, stride, padding)\n",
    "                    #print(op)\n",
    "\n",
    "                    vp = torch.sum(op*grad_out)\n",
    "                    vn = torch.sum(on*grad_out)\n",
    "                    #print(vn)\n",
    "\n",
    "                    grad_img[b,c,h,w] = (vp-vn)/(2*epsilon)\n",
    "                    #print(grad_img)\n",
    "    \n",
    "    #print(i_n.shape)\n",
    "    #print(op.shape)\n",
    "    #print(vn.shape)\n",
    "    #print(grad_img.shape)\n",
    "    \n",
    "    return grad_img, grad_filter\n",
    "\n",
    "\n",
    "def unit_test_grad_checker(img, filter, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
    "    epsilon = 1.0e-5\n",
    "    filter.requires_grad = True\n",
    "    img.requires_grad = True\n",
    "\n",
    "    ### ground truth conv2D\n",
    "    img_out = F.conv2d(img, filter, stride=stride, padding=padding)\n",
    "\n",
    "    # create a random vector v\n",
    "    v = torch.randn_like(img_out)\n",
    "\n",
    "    # call your implemented \"grad_checker\" function\n",
    "    grad_img, grad_filter = grad_checker(img, filter, F.conv2d, v, epsilon=epsilon, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    # compute ground-truth gradients\n",
    "    grad_img_gt = torch.autograd.grad(img_out, img, grad_outputs=v, retain_graph=True)[0]\n",
    "    grad_filter_gt = torch.autograd.grad(img_out, filter, grad_outputs=v, retain_graph=True)[0]\n",
    "    #pdb.set_trace()\n",
    "\n",
    "    diff = (grad_img - grad_img_gt).norm()\n",
    "    if diff < 1.0e-5:\n",
    "        print(\"Your implementation of grad_img is correct!\")\n",
    "    else:\n",
    "        print(\"Your implementation of grad_img is wrong!\")\n",
    "\n",
    "    diff = (grad_filter - grad_filter_gt).norm()\n",
    "    if diff < 1.0e-5:\n",
    "        print(\"Your implementation of grad_filter is correct!\")\n",
    "    else:\n",
    "        print(\"Your implementation of grad_filter is wrong!\")\n",
    "\n",
    "unit_test_grad_checker(img, filter, channel_size=C, num_filters=D, kernel_size=K, stride=1, padding=P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJ3DtRlwUlzY"
   },
   "source": [
    "---\n",
    "#Q2 [5Pts]: Implement ReLU and its gradient\n",
    "\n",
    "**Task**: Implement ReLU operator, i.e., $f(x) = max(x, 0)$, and its gradient operator ${\\frac{\\partial f}{\\partial x}}^{\\top} v$ for any given tensor $v$ that is of the same shape as $x$.\n",
    "\n",
    "**N.B.**: For simplicity, we can assume the input $x$ is of shape $B \\times C \\times H \\times W$ as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_ruiFoTsJb9w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation of func_relu is correct!\n",
      "Your implementation of grad_relu is correct!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tn/2spq72m9385bz56_ft5yg5s80000gn/T/ipykernel_19842/1011957423.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  filt = torch.tensor(filt)\n"
     ]
    }
   ],
   "source": [
    "## implement the following functions\n",
    "def func_relu(x):\n",
    "    ### Fill in this function ###\n",
    "    # Args:\n",
    "    #   x: input, shape B x C x H x W\n",
    "    #\n",
    "    # Returns:\n",
    "    #   y: output, shape B x C x H x W\n",
    "    zeros = torch.zeros_like(x)\n",
    "    y = torch.maximum(x, zeros)\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "def grad_relu(x, y, grad_out):\n",
    "    ### Fill in this function ###\n",
    "    # Args:\n",
    "    #   x: input, shape B x C x H x W\n",
    "    #   y: output, shape B x C x H x W\n",
    "    #   grad_out: gradient w.r.t. output y, shape B x D x H x W\n",
    "    #\n",
    "    # Returns:\n",
    "    #   grad_x: gradient w.r.t. x, shape B x C x H x W\n",
    "    filt = (x>0)\n",
    "    filt = torch.tensor(filt)\n",
    "    grad_x = filt*grad_out\n",
    "    \n",
    "    return grad_x\n",
    "\n",
    "\n",
    "def unit_test_relu(x):\n",
    "    x.requires_grad = True\n",
    "\n",
    "    # call your implemented \"func_relu\" function\n",
    "    y = func_relu(x)\n",
    "\n",
    "    # ground truth ReLU\n",
    "    y_gt = F.relu(x)\n",
    "\n",
    "    diff = (y - y_gt).norm()\n",
    "    if diff < 1.0e-5:\n",
    "        print(\"Your implementation of func_relu is correct!\")\n",
    "    else:\n",
    "        print(\"Your implementation of func_relu is wrong!\")\n",
    "\n",
    "    # create a random vector v\n",
    "    v = torch.randn_like(y)\n",
    "\n",
    "    # call your implemented \"grad_relu\" function\n",
    "    grad_x = grad_relu(x, y, v)\n",
    "\n",
    "    # compute ground-truth gradients\n",
    "    grad_x_gt = torch.autograd.grad(y_gt, x, grad_outputs=v, retain_graph=True)[0]\n",
    "\n",
    "    diff = (grad_x - grad_x_gt).norm()\n",
    "    if diff < 1.0e-5:\n",
    "        print(\"Your implementation of grad_relu is correct!\")\n",
    "    else:\n",
    "        print(\"Your implementation of grad_relu is wrong!\")\n",
    "\n",
    "unit_test_relu(torch.randn_like(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiVr7-xJU066"
   },
   "source": [
    "---\n",
    "#Q3 [20Pts]: Implement Batch-normalization (BN) for convolution and its gradient\n",
    "\n",
    "Given a batch of input images $x$ with shape $B \\times C \\times H \\times W$, we compute a single mean and a single standard deviation per channel as below,\n",
    "\\begin{align}\n",
    "    \\mu[c] &= \\frac{1}{BHW} \\sum_{i=1}^{B} \\sum_{m=1}^{H} \\sum_{n=1}^{W} x[i, c, m, n] \\\\\n",
    "    \\sigma^2[c] &= \\frac{1}{BHW} \\sum_{i=1}^{B} \\sum_{m=1}^{H} \\sum_{n=1}^{W} (x[i, c, m, n] - \\mu[c])^2.\n",
    "\\end{align}\n",
    "Then we perform BN, $y = f(x, \\beta, \\gamma)$, as,\n",
    "\\begin{align}\n",
    "    y[i,c,m,n] &= \\gamma[c] \\frac{x[i,c,m,n] - \\mu[c]}{\\sqrt{\\sigma^2[c] + \\epsilon}} + \\beta[c],\n",
    "\\end{align}\n",
    "where $\\gamma$ and $\\beta$ are learnable parameters are of shape $C$.\n",
    "$ϵ$ is a constant.\n",
    "\n",
    "**Task**: For simplicity, we fix the learnable parameters as $\\gamma = 1$ and $\\beta = 0$.\n",
    "Implement BN for convolution and its gradient operators ${\\frac{\\partial f}{\\partial x}}^{\\top} v$ for any $v$ that is compatible with the matrix multiplication.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3Sou_6LzJcxS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation of func_batch_norm is correct!\n",
      "Your implementation of grad_batch_norm is correct!\n"
     ]
    }
   ],
   "source": [
    "## implement the following functions\n",
    "def func_batch_norm(x, epsilon=1.0e-5):\n",
    "    ### Fill in this function ###\n",
    "    # Args:\n",
    "    #   x: input, shape B x C x H x W\n",
    "    #   epsilon: constant, scalar\n",
    "    #\n",
    "    # Returns:\n",
    "    #   y: output, shape B x C x H x W\n",
    "    \n",
    "    gamma = 1\n",
    "    beta = 0\n",
    "    \n",
    "    mu = mean(x)      \n",
    "    #print(mu)\n",
    "    \n",
    "    sigma = var(x, mu) \n",
    "    #print(sigma)\n",
    "    \n",
    "    y = bn(x, mu, sigma, gamma, beta, epsilon)\n",
    "    #print(y)\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "def mean(x):\n",
    "    # finds and returns the mean per channel\n",
    "    \n",
    "    avg = torch.zeros((x.shape[1],))\n",
    "    for c in range(x.shape[1]):\n",
    "        total = 0.0\n",
    "        for b in range(x.shape[0]):\n",
    "            for h in range(x.shape[2]):\n",
    "                for w in range(x.shape[3]):\n",
    "                    total += x[b,c,h,w]\n",
    "        avg[c] = total/(x.shape[0]*x.shape[2]*x.shape[3])\n",
    "        \n",
    "    return avg\n",
    "\n",
    "\n",
    "def var(x, avg):\n",
    "    # finds and returns the variance per channel\n",
    "    \n",
    "    var = torch.zeros((x.shape[1],))\n",
    "    for c in range(x.shape[1]):\n",
    "        total = 0.0\n",
    "        for b in range(x.shape[0]):\n",
    "            for h in range(x.shape[2]):\n",
    "                for w in range(x.shape[3]):\n",
    "                    total += (x[b,c,h,w]-avg[c])**2\n",
    "        var[c] = total/(x.shape[0]*x.shape[2]*x.shape[3])\n",
    "        \n",
    "    return var\n",
    "\n",
    "\n",
    "def bn(x, avg, var, g, b, e):\n",
    "    # computes batch normalization with learnable parameters and returns it\n",
    "    \n",
    "    y = torch.zeros((x.shape[0],x.shape[1],x.shape[2],x.shape[3]))\n",
    "    for c in range(x.shape[1]):\n",
    "        for i in range(x.shape[0]):\n",
    "            for h in range(x.shape[2]):\n",
    "                for w in range(x.shape[3]):\n",
    "                    y[i,c,h,w] = g*(x[i,c,h,w]-avg[c])/(var[c]+e)**0.5+b\n",
    "                    \n",
    "    return y\n",
    "\n",
    "\n",
    "def grad_batch_norm(x, y, grad_out, epsilon=1.0e-5):\n",
    "    ### Fill in this function ###\n",
    "    # Args:\n",
    "    #   x: input, shape B x C x H x W\n",
    "    #   y: output, shape B x C x H x W\n",
    "    #   grad_out: gradient w.r.t. output y, shape B x D x H x W\n",
    "    #   epsilon: constant, scalar\n",
    "    #\n",
    "    # Returns:\n",
    "    #   grad_x: gradient w.r.t. x, shape B x C x H x W\n",
    "    \n",
    "    gamma = 1.0\n",
    "    mu = mean(x)      \n",
    "    sigma = var(x, mu)\n",
    "    \n",
    "    grad_x = torch.zeros_like(x)\n",
    "    sum_grad_out = torch.zeros((x.shape[1],))\n",
    "    sum_grad_out_xu = torch.zeros((x.shape[1],))\n",
    "    \n",
    "    for c in range(x.shape[1]):\n",
    "        for b in range(x.shape[0]):\n",
    "            for h in range(x.shape[2]):\n",
    "                for w in range(x.shape[3]):\n",
    "                    sum_grad_out[c] += grad_out[b,c,h,w]\n",
    "                    sum_grad_out_xu[c] += grad_out[b,c,h,w]*(x[b,c,h,w]-mu[c])\n",
    "    \n",
    "    for c in range(x.shape[1]):\n",
    "        for b in range(x.shape[0]):\n",
    "            for h in range(x.shape[2]):\n",
    "                for w in range(x.shape[3]):\n",
    "                    term1 = grad_out[b,c,h,w]*gamma\n",
    "                    term2 = -(sum_grad_out[c]*gamma)/(x.shape[0]*x.shape[2]*x.shape[3])\n",
    "                    term3 = -((x[b,c,h,w]-mu[c])*(sum_grad_out_xu[c]*gamma)/(sigma[c]+epsilon))/(x.shape[0]*x.shape[2]*x.shape[3])\n",
    "                    factor = (sigma[c]+epsilon)**-0.5\n",
    "                    grad_x[b,c,h,w] = factor*(term1+term2+term3)\n",
    "    \n",
    "    return grad_x\n",
    "\n",
    "\n",
    "def unit_test_batch_norm(x):\n",
    "    x.requires_grad = True\n",
    "    epsilon = 1e-5\n",
    "\n",
    "    # call your implemented \"func_batch_norm\" function\n",
    "    y = func_batch_norm(x, epsilon=epsilon)\n",
    "\n",
    "    # ground truth ReLU\n",
    "    BN_gt = nn.BatchNorm2d(x.shape[1], eps=epsilon, momentum=1.0, affine=False, track_running_stats=False)\n",
    "    y_gt = BN_gt(x)\n",
    "\n",
    "    diff = (y - y_gt).norm()\n",
    "    if diff < 1.0e-5:\n",
    "        print(\"Your implementation of func_batch_norm is correct!\")\n",
    "    else:\n",
    "        print(\"Your implementation of func_batch_norm is wrong!\")\n",
    "\n",
    "    # create a random vector v\n",
    "    v = torch.randn_like(y)\n",
    "\n",
    "    # call your implemented \"grad_batch_norm\" function\n",
    "    grad_x = grad_batch_norm(x, y, v, epsilon=epsilon)\n",
    "\n",
    "    # compute ground-truth gradients\n",
    "    grad_x_gt = torch.autograd.grad(y_gt, x, grad_outputs=v, retain_graph=True)[0]\n",
    "\n",
    "    diff = (grad_x - grad_x_gt).norm()\n",
    "    if diff < 1.0e-5:\n",
    "        print(\"Your implementation of grad_batch_norm is correct!\")\n",
    "    else:\n",
    "        print(\"Your implementation of grad_batch_norm is wrong!\")\n",
    "        print(grad_x.norm())\n",
    "        print(grad_x_gt.norm())\n",
    "\n",
    "unit_test_batch_norm(torch.randn_like(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2m-KuDI7U2zb"
   },
   "source": [
    "---\n",
    "#Q4 [15Pts]: Implement a simple CNN and back-propagation (BP)\n",
    "\n",
    "Now we are ready to build a deep CNN and learn it with back-propagation.\n",
    "In particular, let us build a simple CNN with following architecture:\n",
    "\n",
    "Conv $→$ BN $→$ ReLU $→$ Conv $→$ BN $→$ ReLU $→$ Linear.\n",
    "\n",
    "Here, for all layers, the convolutions are the same as before (i.e., kernel size $3 \\times 3$, zero-padding, number of filters $D = 2$, and stride 1), the BNs are without learnable $\\gamma$ and $\\beta$, and the last linear layer would map whatever input dimension to $10$ classes in MNIST.\n",
    "\n",
    "**Task**: Implement the above CNN, compute the cross-entropy loss, and compute gradient of the loss w.r.t. filter weights.\n",
    "\n",
    "**N.B.**: You can use F.cross_entropy provided by PyTorch. But for other operators like Conv, BN, and ReLU and their gradietns, you should use your previous implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OnpM4Yj_JdVO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tn/2spq72m9385bz56_ft5yg5s80000gn/T/ipykernel_19842/1011957423.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  filt = torch.tensor(filt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation of grad_filter_1 is correct!\n",
      "Your implementation of grad_filter_2 is correct!\n",
      "Your implementation of grad_weight is correct!\n"
     ]
    }
   ],
   "source": [
    "## implement the following two functions\n",
    "def CNN(img, filter_1, filter_2, weight, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
    "    ### Fill in this function ###\n",
    "    # Args:\n",
    "    #   img: images, shape B x C x H x W\n",
    "    #   filter_1: filters at 1st layer, shape D x C x K x K\n",
    "    #   filter_2: filters at 2nd layer, shape D x D x K x K\n",
    "    #   weight: weights of linear readout layer, shape ? x 10\n",
    "    #   channel_size: number of channels, scalar (C)\n",
    "    #   num_filters: number of filters, scalar (D)\n",
    "    #   kernel_size: kernel size, scalar (K)\n",
    "    #   stride: stride size, scalar\n",
    "    #   padding: padding size, scalar\n",
    "    #\n",
    "    # Returns:\n",
    "    #   out: logits, shape B x 10\n",
    "    \n",
    "    ### 1st layer\n",
    "    c1 = conv2d_im2col(img, filter_1, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    bn1 = func_batch_norm(c1)\n",
    "    r1 = func_relu(bn1)\n",
    "\n",
    "    ### 2nd layer\n",
    "    c2 = conv2d_im2col(r1, filter_2, channel_size=num_filters, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    bn2 = func_batch_norm(c2)\n",
    "    r2 = func_relu(bn2)\n",
    "\n",
    "    ### linear readout\n",
    "    flattened = r2.reshape(r2.shape[0], -1)\n",
    "    out = flattened@weight\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def grad_CNN(img, filter_1, filter_2, weight, grad_loss, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
    "    ### Fill in this function ###\n",
    "    # Args:\n",
    "    #   img: images, shape B x C x H x W\n",
    "    #   filter_1: filters at 1st layer, shape D x C x K x K\n",
    "    #   filter_2: filters at 2nd layer, shape D x D x K x K\n",
    "    #   weight: weights of linear readout layer, shape ? x 10\n",
    "    #   grad_loss: gradient of loss w.r.t. logits, shape B x 10\n",
    "    #   channel_size: number of channels, scalar (C)\n",
    "    #   num_filters: number of filters, scalar (D)\n",
    "    #   kernel_size: kernel size, scalar (K)\n",
    "    #   stride: stride size, scalar\n",
    "    #   padding: padding size, scalar\n",
    "    #\n",
    "    # Returns:\n",
    "    #   grad_filter_1: filters, shape D x C x K x K\n",
    "    #   grad_filter_2: filters, shape D x C x K x K\n",
    "    #   grad_weight: weight, shape ? x 10\n",
    "\n",
    "    ### 1st layer\n",
    "    c1 = conv2d_im2col(img, filter_1, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    bn1 = func_batch_norm(c1)\n",
    "    r1 = func_relu(bn1)\n",
    "\n",
    "    ### 2nd layer\n",
    "    c2 = conv2d_im2col(r1, filter_2, channel_size=num_filters, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    bn2 = func_batch_norm(c2)\n",
    "    r2 = func_relu(bn2)\n",
    "\n",
    "    ### linear readout\n",
    "    flattened = r2.reshape(r2.shape[0], -1)\n",
    "    #out = flattened@weight\n",
    "    \n",
    "    grad_weight = flattened.t()@grad_loss\n",
    "    grad_flattened = grad_loss@weight.t()\n",
    "\n",
    "    grad_r2 = grad_flattened.reshape(r2.shape)\n",
    "    grad_bn2 = grad_relu(bn2, r2, grad_r2)\n",
    "    grad_c2 = grad_batch_norm(c2, bn2, grad_bn2)\n",
    "\n",
    "    grad_r1, grad_filter_2 = grad_conv2d(r1, filter_2, c2, grad_c2, channel_size=num_filters, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    grad_bn1 = grad_relu(bn1, r1, grad_r1)\n",
    "    grad_c1 = grad_batch_norm(c1, bn1, grad_bn1)\n",
    "\n",
    "    grad_img, grad_filter_1 = grad_conv2d(img, filter_1, c1, grad_c1, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    \n",
    "    return grad_filter_1, grad_filter_2, grad_weight\n",
    "    \n",
    "\n",
    "def unit_test_CNN(img, label, filter_1, filter_2, weight, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
    "    # call your implemented \"CNN\"\n",
    "    img.requires_grad_()\n",
    "    filter_1.requires_grad_()\n",
    "    filter_2.requires_grad_()\n",
    "    weight.requires_grad_()\n",
    "    y = CNN(img, filter_1, filter_2, weight, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    y.requires_grad_()\n",
    "\n",
    "    # compute loss function\n",
    "    loss = F.cross_entropy(y, label).mean()\n",
    "    loss.requires_grad_()\n",
    "\n",
    "    # compute gradient of loss w.r.t. logits\n",
    "    grad_loss = torch.autograd.grad(loss, y, retain_graph=True)[0]\n",
    "\n",
    "    # call your implemented \"grad_batch_norm\" function\n",
    "    grad_filter_1, grad_filter_2, grad_weight = grad_CNN(img, filter_1, filter_2, weight, grad_loss, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    # compute ground-truth gradients\n",
    "    grad_filter_1_gt = torch.autograd.grad(loss, filter_1, retain_graph=True)[0]\n",
    "    grad_filter_2_gt = torch.autograd.grad(loss, filter_2, retain_graph=True)[0]\n",
    "    grad_weight_gt = torch.autograd.grad(loss, weight, retain_graph=True)[0]\n",
    "\n",
    "    diff = (grad_filter_1 - grad_filter_1_gt).norm()\n",
    "    if diff < 1.0e-5:\n",
    "        print(\"Your implementation of grad_filter_1 is correct!\")\n",
    "    else:\n",
    "        print(\"Your implementation of grad_filter_1 is wrong!\")\n",
    "\n",
    "    diff = (grad_filter_2 - grad_filter_2_gt).norm()\n",
    "    if diff < 1.0e-5:\n",
    "        print(\"Your implementation of grad_filter_2 is correct!\")\n",
    "    else:\n",
    "        print(\"Your implementation of grad_filter_2 is wrong!\")\n",
    "\n",
    "    diff = (grad_weight - grad_weight_gt).norm()\n",
    "    if diff < 1.0e-5:\n",
    "        print(\"Your implementation of grad_weight is correct!\")\n",
    "    else:\n",
    "        print(\"Your implementation of grad_weight is wrong!\")\n",
    "\n",
    "\n",
    "filter_1 = torch.randn(D, C, K, K) # filter shape: D x C x K x K\n",
    "filter_2 = torch.randn(D, D, K, K) # filter shape: D x C x K x K\n",
    "\n",
    "### compute the correct shape and then replace None with it ###\n",
    "weight = torch.randn(D*img.shape[2]*img.shape[3], 10) # weight of the last linear layer\n",
    "#print(weight.shape)\n",
    "\n",
    "unit_test_CNN(img, label, filter_1, filter_2, weight, channel_size=C, num_filters=D, kernel_size=K, stride=1, padding=P)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9AS6zKmYSwVD"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
