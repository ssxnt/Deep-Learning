{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJmxtD-kg8KE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eritefi9hPgk"
      },
      "outputs": [],
      "source": [
        "class SubstringDataset(Dataset):\n",
        "    LETTERS = list('cpen')\n",
        "\n",
        "    def __init__(self, seed, dataset_size, str_len=20):\n",
        "        super().__init__()\n",
        "        self.str_len = str_len\n",
        "        self.dataset_size = dataset_size\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.strings, self.labels = self._create_dataset()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.strings[index], self.labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "    def _create_dataset(self):\n",
        "        strings, labels = [], []\n",
        "        for i in range(self.dataset_size):\n",
        "            label = i%2\n",
        "            string = self._generate_random_string(bool(label))\n",
        "            strings.append(string)\n",
        "            labels.append(label)\n",
        "        return strings, labels\n",
        "\n",
        "    def _generate_random_string(self, has_cpen):\n",
        "        while True:\n",
        "            st = ''.join(self.rng.choice(SubstringDataset.LETTERS, size=self.str_len))\n",
        "            if ('cpen' in st) == has_cpen:\n",
        "                return st"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY91aQytmtqK"
      },
      "outputs": [],
      "source": [
        "class Tokenizer():\n",
        "    def __init__(self) -> None:\n",
        "        self.vocab = {\n",
        "            '[CLS]': 0,\n",
        "            'c': 1,\n",
        "            'p': 2,\n",
        "            'e': 3,\n",
        "            'n': 4,\n",
        "        }\n",
        "\n",
        "    def tokenize_string(self, string, add_cls_token=True) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Tokenize the input string according to the above vocab\n",
        "\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "\n",
        "        tokenized_string = None\n",
        "\n",
        "        cls = next(iter(self.vocab))\n",
        "\n",
        "        vocab_length = len(self.vocab)\n",
        "\n",
        "        tokens = list()\n",
        "        indices = list()\n",
        "        ohv = list()\n",
        "\n",
        "        if (add_cls_token):\n",
        "            tokens.append(cls)\n",
        "\n",
        "        for s in string:\n",
        "            tokens.append(s)\n",
        "\n",
        "        for t in tokens:\n",
        "            indices.append(self.vocab[t])\n",
        "\n",
        "        for i in indices:\n",
        "            oh = torch.zeros(vocab_length)\n",
        "            oh[i] = 1\n",
        "            ohv.append(oh)\n",
        "\n",
        "        tokenized_string = torch.stack(ohv)\n",
        "\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return tokenized_string\n",
        "\n",
        "    def tokenize_string_batch(self, strings, add_cls_token=True):\n",
        "        X = []\n",
        "        for s in strings:\n",
        "            X.append(self.tokenize_string(s, add_cls_token=add_cls_token))\n",
        "        return torch.stack(X, dim=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG_mI9VSj0XM"
      },
      "outputs": [],
      "source": [
        "class AbsolutePositionalEncoding(nn.Module):\n",
        "    MAX_LEN = 256\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.empty((self.MAX_LEN, d_model)))\n",
        "        nn.init.normal_(self.W)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            x: shape B x N x D\n",
        "        returns:\n",
        "            out: shape B x N x D\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "\n",
        "        out = None\n",
        "\n",
        "        xpw = torch.zeros(x.size(), device=x.device)\n",
        "\n",
        "        pos_enc = self.W[:x.size(1)].unsqueeze(0)\n",
        "        xpw = x+pos_enc\n",
        "\n",
        "        out = xpw\n",
        "\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    MAX_LEN = 256\n",
        "\n",
        "    def __init__(self, d_model, n_heads, rpe):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"Number of heads must divide number of dimensions\"\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_h = d_model // n_heads\n",
        "        self.rpe = rpe\n",
        "        self.Wq = nn.ParameterList([nn.Parameter(torch.empty((d_model, self.d_h))) for _ in range(n_heads)])\n",
        "        self.Wk = nn.ParameterList([nn.Parameter(torch.empty((d_model, self.d_h))) for _ in range(n_heads)])\n",
        "        self.Wv = nn.ParameterList([nn.Parameter(torch.empty((d_model, self.d_h))) for _ in range(n_heads)])\n",
        "        self.Wo = nn.Parameter(torch.empty((d_model, d_model)))\n",
        "\n",
        "        if rpe:\n",
        "            # -MAX_LEN, -MAX_LEN+1, ..., -1, 0, 1, ..., MAX_LEN-1, MAXLEN\n",
        "            self.rpe_w = nn.ParameterList([nn.Parameter(torch.empty((2*self.MAX_LEN+1, ))) for _ in range(n_heads)])\n",
        "\n",
        "        for h in range(self.n_heads):\n",
        "            nn.init.xavier_normal_(self.Wk[h])\n",
        "            nn.init.xavier_normal_(self.Wq[h])\n",
        "            nn.init.xavier_normal_(self.Wv[h])\n",
        "            if rpe:\n",
        "                nn.init.normal_(self.rpe_w[h])\n",
        "        nn.init.xavier_normal_(self.Wo)\n",
        "\n",
        "    def forward(self, key, query, value):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            key: shape B x N x D\n",
        "            query: shape B x N x D\n",
        "            value: shape B x N x D\n",
        "        return:\n",
        "            out: shape B x N x D\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "\n",
        "        out = None\n",
        "\n",
        "        def compute_attention_weights(rpe=True):\n",
        "            head_h = list()\n",
        "            for h in range(self.n_heads):\n",
        "                xq_wqh = query@self.Wq[h]\n",
        "                xk_wkh = key@self.Wk[h]\n",
        "                xk_wkh_t = xk_wkh.transpose(-1, -2)\n",
        "                xv_wvh = value@self.Wv[h]\n",
        "                term1 = xq_wqh@xk_wkh_t\n",
        "                if (rpe):\n",
        "                    pos_indices = torch.arange(query.shape[1], device=query.device)\n",
        "                    pos_indices = pos_indices.unsqueeze(0)-pos_indices.unsqueeze(1)\n",
        "                    pos_indices += self.MAX_LEN\n",
        "                    mh = self.rpe_w[h][pos_indices]\n",
        "                    term1 += mh.unsqueeze(0)\n",
        "\n",
        "                term1 = term1/(self.d_h**0.5)\n",
        "                attn = F.softmax(term1, dim=-1)\n",
        "                head = attn@xv_wvh\n",
        "                head_h.append(head)\n",
        "\n",
        "            return head_h\n",
        "\n",
        "        if (self.rpe):\n",
        "            head_h = compute_attention_weights()\n",
        "        else:\n",
        "            head_h = compute_attention_weights(rpe=False)\n",
        "\n",
        "        concats = torch.cat(head_h, dim=2)\n",
        "        out = concats@self.Wo\n",
        "\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iCqSqRDsGw3"
      },
      "outputs": [],
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, prenorm: bool, rpe: bool):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.prenorm = prenorm\n",
        "        self.attention = MultiHeadAttention(d_model, n_heads, rpe=rpe)\n",
        "        self.fc_W1 = nn.Parameter(torch.empty((d_model, 4*d_model)))\n",
        "        self.fc_W2 = nn.Parameter(torch.empty((4*d_model, d_model)))\n",
        "        self.relu = nn.ReLU()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        nn.init.xavier_normal_(self.fc_W1)\n",
        "        nn.init.xavier_normal_(self.fc_W2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            x: shape B x N x D\n",
        "        returns:\n",
        "            out: shape B x N x D\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "\n",
        "        out = None\n",
        "\n",
        "        if (self.prenorm):\n",
        "            nl1 = self.ln1(x)\n",
        "            mha = self.attention(nl1, nl1, nl1)\n",
        "            res1 = x+mha\n",
        "            nl2 = self.ln2(res1)\n",
        "            ffn = self.relu(nl2@self.fc_W1)@self.fc_W2\n",
        "            out = res1+ffn\n",
        "        else:\n",
        "            mha = self.attention(x, x, x)\n",
        "            res1 = x+mha\n",
        "            nl1 = self.ln1(res1)\n",
        "            ffn = self.relu(nl1@self.fc_W1)@self.fc_W2\n",
        "            res2 = nl1+ffn\n",
        "            out = self.ln2(res2)\n",
        "\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxPXjEj1ydLf"
      },
      "outputs": [],
      "source": [
        "class ModelConfig:\n",
        "    n_layers = 4\n",
        "    input_dim = 5\n",
        "    d_model = 256\n",
        "    n_heads = 4\n",
        "    prenorm = True\n",
        "    pos_enc_type = 'ape' # 'ape': Abosolute Pos. Enc., 'rpe': Relative Pos. Enc.\n",
        "    output_dim = 1 # Binary output: 0: invalid, 1: valid\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            assert hasattr(self, k)\n",
        "            self.__setattr__(k, v)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, cfg: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.enc_W = nn.Parameter(torch.empty((cfg.input_dim, cfg.d_model)))\n",
        "        if cfg.pos_enc_type == 'ape':\n",
        "            self.ape = AbsolutePositionalEncoding(d_model=cfg.d_model)\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerLayer(d_model=cfg.d_model, n_heads=cfg.n_heads, prenorm=cfg.prenorm, rpe=cfg.pos_enc_type == 'rpe') for _ in range(cfg.n_layers)\n",
        "        ])\n",
        "        self.dec_W = nn.Parameter(torch.empty((cfg.d_model, cfg.output_dim)))\n",
        "\n",
        "        nn.init.xavier_normal_(self.enc_W)\n",
        "        nn.init.xavier_normal_(self.dec_W)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            x: shape B x N x D_in\n",
        "        returns:\n",
        "            out: shape B x N x D_out\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "\n",
        "        out = None\n",
        "\n",
        "        h = x@self.enc_W\n",
        "\n",
        "        if (self.cfg.pos_enc_type == 'ape'):\n",
        "            h = self.ape(h)\n",
        "\n",
        "        for layer in self.transformer_layers:\n",
        "            h = layer(h)\n",
        "\n",
        "        out = h@self.dec_W\n",
        "\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k32Ps5WS9rg-"
      },
      "outputs": [],
      "source": [
        "from torch.optim import lr_scheduler\n",
        "\n",
        "class CustomScheduler(lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, total_steps, warmup_steps=1000):\n",
        "        self.total_steps = total_steps\n",
        "        self.warmup_steps = warmup_steps\n",
        "        super().__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        \"\"\"\n",
        "        Compute the custom scheduler with warmup and cooldown\n",
        "        Hint: self.last_epoch contains the current step number\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "\n",
        "        mult_factor = 1.0\n",
        "\n",
        "        if self.last_epoch < self.warmup_steps:\n",
        "            mult_factor = self.last_epoch/self.warmup_steps\n",
        "        else:\n",
        "            mult_factor = (self.total_steps-self.last_epoch)/(self.total_steps-self.warmup_steps)\n",
        "\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return [group['initial_lr'] * mult_factor for group in self.optimizer.param_groups]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmjFKAXcyeZm"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TrainerConfig:\n",
        "    lr = 0.003\n",
        "    train_steps = 5000\n",
        "    batch_size = 256\n",
        "    evaluate_every = 100\n",
        "    device = 'cpu'\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            assert hasattr(self, k)\n",
        "            self.__setattr__(k, v)\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, cfg: TrainerConfig):\n",
        "        self.cfg = cfg\n",
        "        self.device = cfg.device\n",
        "        self.tokenizer = Tokenizer()\n",
        "        self.model = model.to(self.device)\n",
        "\n",
        "    def train(self, train_dataset, val_dataset):\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.cfg.lr)\n",
        "        scheduler = CustomScheduler(optimizer, self.cfg.train_steps)\n",
        "        train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=self.cfg.batch_size)\n",
        "        for step in range(self.cfg.train_steps):\n",
        "            self.model.train()\n",
        "            batch = next(iter(train_dataloader))\n",
        "            strings, y = batch\n",
        "            x = self.tokenizer.tokenize_string_batch(strings)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss, _ = self.compute_batch_loss_acc(x, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            if step % self.cfg.evaluate_every == 0:\n",
        "                val_loss, val_acc = self.evaluate_dataset(val_dataset)\n",
        "                print(f\"Step {step}: Train Loss={loss.item()}, Val Loss: {val_loss}, Val Accuracy: {val_acc}\")\n",
        "\n",
        "    def compute_batch_loss_acc(self, x, y):\n",
        "        \"\"\"\n",
        "        Compute the loss and accuracy of the model on batch (x, y)\n",
        "        args:\n",
        "            x: B x N x D_in\n",
        "            y: B\n",
        "        return:\n",
        "            loss, accuracy\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "\n",
        "        loss, acc = torch.tensor([1.0]), torch.tensor([0.0])\n",
        "\n",
        "        x = x.to(self.device)\n",
        "        y = y.to(self.device).float()\n",
        "\n",
        "        cls_out = self.model(x)[:, 0, :].squeeze()\n",
        "\n",
        "        preds = torch.round(torch.sigmoid(cls_out))\n",
        "        acc = (preds == y).float().mean()\n",
        "\n",
        "        bce_loss = nn.BCEWithLogitsLoss()\n",
        "        loss = bce_loss(cls_out, y)\n",
        "\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return loss, acc\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate_dataset(self, dataset):\n",
        "        self.model.eval()\n",
        "        dataloader = DataLoader(dataset, shuffle=False, batch_size=self.cfg.batch_size)\n",
        "        final_loss, final_acc = 0.0, 0.0\n",
        "        for batch in dataloader:\n",
        "            strings, y = batch\n",
        "            x = self.tokenizer.tokenize_string_batch(strings)\n",
        "            loss, acc = self.compute_batch_loss_acc(x, y)\n",
        "            final_loss += loss.item() * x.size(0)\n",
        "            final_acc += acc.item() * x.size(0)\n",
        "        return final_loss / len(dataset), final_acc / len(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5zfy4SVFy0V"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "In case you were not successful in implementing some of the above classes,\n",
        "you may reimplement them using pytorch available nn Modules here to receive the marks for part 1.8\n",
        "If your implementation of the previous parts is correct, leave this block empty.\n",
        "START BLOCK\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "END BLOCK\n",
        "\"\"\"\n",
        "def run_transformer():\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"using {device}\")\n",
        "    model = TransformerModel(ModelConfig())\n",
        "    trainer = Trainer(model, TrainerConfig(device=device))\n",
        "    parantheses_size=16\n",
        "    print(\"Creating datasets.\")\n",
        "    train_dataset = SubstringDataset(seed=1, dataset_size=10_000, str_len=parantheses_size)\n",
        "    val_dataset = SubstringDataset(seed=2, dataset_size=1_000, str_len=parantheses_size)\n",
        "    test_dataset = SubstringDataset(seed=3, dataset_size=1_000, str_len=parantheses_size)\n",
        "\n",
        "    print(\"Training the model.\")\n",
        "    trainer.train(train_dataset, val_dataset)\n",
        "    test_loss, test_acc = trainer.evaluate_dataset(test_dataset)\n",
        "    print(f\"Final Test Accuracy={test_acc}, Test Loss={test_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhAUyeO5F27T"
      },
      "outputs": [],
      "source": [
        "run_transformer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjNEOPRMsGKR"
      },
      "source": [
        "# Unit Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjRY9u_UsFNm"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def seed_all():\n",
        "    torch.manual_seed(0)\n",
        "    random.seed(0)\n",
        "    np.random.seed(0)\n",
        "\n",
        "class TransformerUnitTest:\n",
        "    def __init__(self, gt_vars: dict, verbose=True):\n",
        "        self.gt_vars = gt_vars\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def test_all(self):\n",
        "        self.test_tokenizer()\n",
        "        self.test_ape()\n",
        "        self.test_mha()\n",
        "        self.test_transformer_layer()\n",
        "        self.test_transformer_model()\n",
        "        self.test_scheduler()\n",
        "        self.test_loss()\n",
        "\n",
        "    def test_tokenizer(self):\n",
        "        seed_all()\n",
        "        self.check_correctness(\n",
        "            Tokenizer().tokenize_string('ccpeen', add_cls_token=True),\n",
        "            self.gt_vars['tokenizer_1'],\n",
        "            \"Tokenization with cls class\"\n",
        "        )\n",
        "        self.check_correctness(\n",
        "            Tokenizer().tokenize_string('cpppencpen', add_cls_token=False),\n",
        "            self.gt_vars['tokenizer_2'],\n",
        "            \"Tokenization without cls class\"\n",
        "        )\n",
        "\n",
        "    def test_ape(self):\n",
        "        seed_all()\n",
        "        ape_result = AbsolutePositionalEncoding(128)(torch.randn((8, 12, 128)))\n",
        "        self.check_correctness(ape_result, self.gt_vars['ape'], \"APE\")\n",
        "\n",
        "    def test_mha(self):\n",
        "        seed_all()\n",
        "        mha_result = MultiHeadAttention(d_model=128, n_heads=4, rpe=False)(\n",
        "            torch.randn((8, 12, 128)), torch.randn((8, 12, 128)), torch.randn((8, 12, 128))\n",
        "        )\n",
        "        self.check_correctness(\n",
        "            mha_result,\n",
        "            self.gt_vars['mha_no_rpe'],\n",
        "            \"Multi-head Attention without RPE\"\n",
        "        )\n",
        "        mha_result_rpe = MultiHeadAttention(d_model=128, n_heads=8, rpe=True)(\n",
        "            torch.randn((8, 12, 128)), torch.randn((8, 12, 128)), torch.randn((8, 12, 128))\n",
        "        )\n",
        "        self.check_correctness(\n",
        "            mha_result_rpe,\n",
        "            self.gt_vars['mha_with_rpe'],\n",
        "            \"Multi-head Attention with RPE\"\n",
        "        )\n",
        "\n",
        "    def test_transformer_layer(self):\n",
        "        seed_all()\n",
        "        for prenorm in [True, False]:\n",
        "            transformer_layer_result = TransformerLayer(\n",
        "                d_model=128, n_heads=4, prenorm=prenorm, rpe=False\n",
        "            )(torch.randn((8, 12, 128)))\n",
        "            self.check_correctness(\n",
        "                transformer_layer_result,\n",
        "                self.gt_vars[f'transformer_layer_prenorm_{prenorm}'],\n",
        "                f\"Transformer Layer Prenorm {prenorm}\"\n",
        "            )\n",
        "\n",
        "    def test_transformer_model(self):\n",
        "        seed_all()\n",
        "        transformer_model_result = TransformerModel(\n",
        "            ModelConfig(d_model=128, prenorm=True, pos_enc_type='ape')\n",
        "        )(torch.randn((8, 12, 5)))\n",
        "        self.check_correctness(\n",
        "            transformer_model_result,\n",
        "            self.gt_vars['transformer_model_result'],\n",
        "            f\"Transformer Model\"\n",
        "        )\n",
        "\n",
        "    def test_scheduler(self):\n",
        "        model = TransformerModel(ModelConfig())\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        scheduler = CustomScheduler(optimizer, 10_000)\n",
        "        optimizer.step()\n",
        "        scheduler.step(521)\n",
        "        self.check_correctness(\n",
        "            torch.tensor([optimizer.param_groups[0]['lr']]),\n",
        "            self.gt_vars['scheduler_1'],\n",
        "            f\"Scheduler Warmup\"\n",
        "        )\n",
        "        scheduler.step(2503)\n",
        "        self.check_correctness(\n",
        "            torch.tensor([optimizer.param_groups[0]['lr']]),\n",
        "            self.gt_vars['scheduler_2'],\n",
        "            f\"Scheduler Cooldown\"\n",
        "        )\n",
        "\n",
        "    def test_loss(self):\n",
        "        seed_all()\n",
        "        model = TransformerModel(ModelConfig())\n",
        "        trainer = Trainer(model, TrainerConfig(device='cpu'))\n",
        "        loss_result, _ = trainer.compute_batch_loss_acc(\n",
        "            torch.randn((8, 12, 5)),\n",
        "            torch.ones(8).float(),\n",
        "        )\n",
        "        self.check_correctness(\n",
        "            loss_result,\n",
        "            self.gt_vars['loss'],\n",
        "            f\"Batch Loss\"\n",
        "        )\n",
        "\n",
        "    def check_correctness(self, out, gt, title):\n",
        "        try:\n",
        "            diff = (out - gt).norm()\n",
        "        except:\n",
        "            diff = float('inf')\n",
        "        if diff < 1e-4:\n",
        "            print(f\"[Correct] {title}\")\n",
        "        else:\n",
        "            print(f\"[Wrong] {title}\")\n",
        "            if self.verbose:\n",
        "                print(\"-----\")\n",
        "                print(\"Expected: \")\n",
        "                print(gt)\n",
        "                print(\"Received: \")\n",
        "                print(out)\n",
        "                print(\"-----\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2DlMVJ4wMrp"
      },
      "outputs": [],
      "source": [
        "!gdown 1-2-__6AALEfqhfew3sJ2QiCE1-rrFMnQ -q -O unit_tests.pkl\n",
        "import pickle\n",
        "with open('unit_tests.pkl', 'rb') as f:\n",
        "    gt_vars = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Enp2ArbjOHEt"
      },
      "outputs": [],
      "source": [
        "TransformerUnitTest(gt_vars, verbose=True).test_all()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}