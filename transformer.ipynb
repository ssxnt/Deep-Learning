{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fJmxtD-kg8KE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sant/anaconda3/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Eritefi9hPgk"
   },
   "outputs": [],
   "source": [
    "class SubstringDataset(Dataset):\n",
    "    LETTERS = list('cpen')\n",
    "\n",
    "    def __init__(self, seed, dataset_size, str_len=20):\n",
    "        super().__init__()\n",
    "        self.str_len = str_len\n",
    "        self.dataset_size = dataset_size\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.strings, self.labels = self._create_dataset()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.strings[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def _create_dataset(self):\n",
    "        strings, labels = [], []\n",
    "        for i in range(self.dataset_size):\n",
    "            label = i%2\n",
    "            string = self._generate_random_string(bool(label))\n",
    "            strings.append(string)\n",
    "            labels.append(label)\n",
    "        return strings, labels\n",
    "\n",
    "    def _generate_random_string(self, has_cpen):\n",
    "        while True:\n",
    "            st = ''.join(self.rng.choice(SubstringDataset.LETTERS, size=self.str_len))\n",
    "            if ('cpen' in st) == has_cpen:\n",
    "                return st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AY91aQytmtqK"
   },
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self) -> None:\n",
    "        self.vocab = {\n",
    "            '[CLS]': 0,\n",
    "            'c': 1,\n",
    "            'p': 2,\n",
    "            'e': 3,\n",
    "            'n': 4,\n",
    "        }\n",
    "\n",
    "    def tokenize_string(self, string, add_cls_token=True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Tokenize the input string according to the above vocab\n",
    "\n",
    "        START BLOCK\n",
    "        \"\"\"\n",
    "        tokenized_string = None\n",
    "        \"\"\"\n",
    "        END BLOCK\n",
    "        \"\"\"\n",
    "        return tokenized_string\n",
    "\n",
    "    def tokenize_string_batch(self, strings, add_cls_token=True):\n",
    "        X = []\n",
    "        for s in strings:\n",
    "            X.append(self.tokenize_string(s, add_cls_token=add_cls_token))\n",
    "        return torch.stack(X, dim=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AG_mI9VSj0XM"
   },
   "outputs": [],
   "source": [
    "class AbsolutePositionalEncoding(nn.Module):\n",
    "    MAX_LEN = 256\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.empty((self.MAX_LEN, d_model)))\n",
    "        nn.init.normal_(self.W)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            x: shape B x N x D\n",
    "        returns:\n",
    "            out: shape B x N x D\n",
    "        START BLOCK\n",
    "        \"\"\"\n",
    "        out = None\n",
    "        \"\"\"\n",
    "        END BLOCK\n",
    "        \"\"\"\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    MAX_LEN = 256\n",
    "\n",
    "    def __init__(self, d_model, n_heads, rpe):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"Number of heads must divide number of dimensions\"\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_h = d_model // n_heads\n",
    "        self.rpe = rpe\n",
    "        self.Wq = nn.ParameterList([nn.Parameter(torch.empty((d_model, self.d_h))) for _ in range(n_heads)])\n",
    "        self.Wk = nn.ParameterList([nn.Parameter(torch.empty((d_model, self.d_h))) for _ in range(n_heads)])\n",
    "        self.Wv = nn.ParameterList([nn.Parameter(torch.empty((d_model, self.d_h))) for _ in range(n_heads)])\n",
    "        self.Wo = nn.Parameter(torch.empty((d_model, d_model)))\n",
    "\n",
    "        if rpe:\n",
    "            # -MAX_LEN, -MAX_LEN+1, ..., -1, 0, 1, ..., MAX_LEN-1, MAXLEN\n",
    "            self.rpe_w = nn.ParameterList([nn.Parameter(torch.empty((2*self.MAX_LEN+1, ))) for _ in range(n_heads)])\n",
    "\n",
    "        for h in range(self.n_heads):\n",
    "            nn.init.xavier_normal_(self.Wk[h])\n",
    "            nn.init.xavier_normal_(self.Wq[h])\n",
    "            nn.init.xavier_normal_(self.Wv[h])\n",
    "            if rpe:\n",
    "                nn.init.normal_(self.rpe_w[h])\n",
    "        nn.init.xavier_normal_(self.Wo)\n",
    "\n",
    "    def forward(self, key, query, value):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            key: shape B x N x D\n",
    "            query: shape B x N x D\n",
    "            value: shape B x N x D\n",
    "        return:\n",
    "            out: shape B x N x D\n",
    "        START BLOCK\n",
    "        \"\"\"\n",
    "        out = None\n",
    "        \"\"\"\n",
    "        END BLOCK\n",
    "        \"\"\"\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4iCqSqRDsGw3"
   },
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, prenorm: bool, rpe: bool):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.prenorm = prenorm\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, rpe=rpe)\n",
    "        self.fc_W1 = nn.Parameter(torch.empty((d_model, 4*d_model)))\n",
    "        self.fc_W2 = nn.Parameter(torch.empty((4*d_model, d_model)))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc_W1)\n",
    "        nn.init.xavier_normal_(self.fc_W2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            x: shape B x N x D\n",
    "        returns:\n",
    "            out: shape B x N x D\n",
    "        START BLOCK\n",
    "        \"\"\"\n",
    "        out = None\n",
    "        \"\"\"\n",
    "        END BLOCK\n",
    "        \"\"\"\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IxPXjEj1ydLf"
   },
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    n_layers = 4\n",
    "    input_dim = 5\n",
    "    d_model = 256\n",
    "    n_heads = 4\n",
    "    prenorm = True\n",
    "    pos_enc_type = 'ape' # 'ape': Abosolute Pos. Enc., 'rpe': Relative Pos. Enc.\n",
    "    output_dim = 1 # Binary output: 0: invalid, 1: valid\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            assert hasattr(self, k)\n",
    "            self.__setattr__(k, v)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, cfg: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.enc_W = nn.Parameter(torch.empty((cfg.input_dim, cfg.d_model)))\n",
    "        if cfg.pos_enc_type == 'ape':\n",
    "            self.ape = AbsolutePositionalEncoding(d_model=cfg.d_model)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerLayer(d_model=cfg.d_model, n_heads=cfg.n_heads, prenorm=cfg.prenorm, rpe=cfg.pos_enc_type == 'rpe') for _ in range(cfg.n_layers)\n",
    "        ])\n",
    "        self.dec_W = nn.Parameter(torch.empty((cfg.d_model, cfg.output_dim)))\n",
    "\n",
    "        nn.init.xavier_normal_(self.enc_W)\n",
    "        nn.init.xavier_normal_(self.dec_W)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            x: shape B x N x D_in\n",
    "        returns:\n",
    "            out: shape B x N x D_out\n",
    "        START BLOCK\n",
    "        \"\"\"\n",
    "        out = None\n",
    "        \"\"\"\n",
    "        END BLOCK\n",
    "        \"\"\"\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "k32Ps5WS9rg-"
   },
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "class CustomScheduler(lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, total_steps, warmup_steps=1000):\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"\n",
    "        Compute the custom scheduler with warmup and cooldown\n",
    "        Hint: self.last_epoch contains the current step number\n",
    "        START BLOCK\n",
    "        \"\"\"\n",
    "        mult_factor = 1.0\n",
    "        \"\"\"\n",
    "        END BLOCK\n",
    "        \"\"\"\n",
    "        return [group['initial_lr'] * mult_factor for group in self.optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HmjFKAXcyeZm"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TrainerConfig:\n",
    "    lr = 0.003\n",
    "    train_steps = 5000\n",
    "    batch_size = 256\n",
    "    evaluate_every = 100\n",
    "    device = 'cpu'\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            assert hasattr(self, k)\n",
    "            self.__setattr__(k, v)\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, cfg: TrainerConfig):\n",
    "        self.cfg = cfg\n",
    "        self.device = cfg.device\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.model = model.to(self.device)\n",
    "\n",
    "    def train(self, train_dataset, val_dataset):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.cfg.lr)\n",
    "        scheduler = CustomScheduler(optimizer, self.cfg.train_steps)\n",
    "        train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=self.cfg.batch_size)\n",
    "        for step in range(self.cfg.train_steps):\n",
    "            self.model.train()\n",
    "            batch = next(iter(train_dataloader))\n",
    "            strings, y = batch\n",
    "            x = self.tokenizer.tokenize_string_batch(strings)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, _ = self.compute_batch_loss_acc(x, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if step % self.cfg.evaluate_every == 0:\n",
    "                val_loss, val_acc = self.evaluate_dataset(val_dataset)\n",
    "                print(f\"Step {step}: Train Loss={loss.item()}, Val Loss: {val_loss}, Val Accuracy: {val_acc}\")\n",
    "\n",
    "    def compute_batch_loss_acc(self, x, y):\n",
    "        \"\"\"\n",
    "        Compute the loss and accuracy of the model on batch (x, y)\n",
    "        args:\n",
    "            x: B x N x D_in\n",
    "            y: B\n",
    "        return:\n",
    "            loss, accuracy\n",
    "        START BLOCK\n",
    "        \"\"\"\n",
    "        loss, acc = torch.tensor([1.0]), torch.tensor([0.0])\n",
    "        \"\"\"\n",
    "        END BLOCK\n",
    "        \"\"\"\n",
    "        return loss, acc\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_dataset(self, dataset):\n",
    "        self.model.eval()\n",
    "        dataloader = DataLoader(dataset, shuffle=False, batch_size=self.cfg.batch_size)\n",
    "        final_loss, final_acc = 0.0, 0.0\n",
    "        for batch in dataloader:\n",
    "            strings, y = batch\n",
    "            x = self.tokenizer.tokenize_string_batch(strings)\n",
    "            loss, acc = self.compute_batch_loss_acc(x, y)\n",
    "            final_loss += loss.item() * x.size(0)\n",
    "            final_acc += acc.item() * x.size(0)\n",
    "        return final_loss / len(dataset), final_acc / len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "b5zfy4SVFy0V"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In case you were not successful in implementing some of the above classes,\n",
    "you may reimplement them using pytorch available nn Modules here to receive the marks for part 1.8\n",
    "If your implementation of the previous parts is correct, leave this block empty.\n",
    "START BLOCK\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "END BLOCK\n",
    "\"\"\"\n",
    "def run_transformer():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = TransformerModel(ModelConfig())\n",
    "    trainer = Trainer(model, TrainerConfig(device=device))\n",
    "    parantheses_size=16\n",
    "    print(\"Creating datasets.\")\n",
    "    train_dataset = SubstringDataset(seed=1, dataset_size=10_000, str_len=parantheses_size)\n",
    "    val_dataset = SubstringDataset(seed=2, dataset_size=1_000, str_len=parantheses_size)\n",
    "    test_dataset = SubstringDataset(seed=3, dataset_size=1_000, str_len=parantheses_size)\n",
    "\n",
    "    print(\"Training the model.\")\n",
    "    trainer.train(train_dataset, val_dataset)\n",
    "    test_loss, test_acc = trainer.evaluate_dataset(test_dataset)\n",
    "    print(f\"Final Test Accuracy={test_acc}, Test Loss={test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IhAUyeO5F27T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets.\n",
      "Training the model.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run_transformer()\n",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m, in \u001b[0;36mrun_transformer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m SubstringDataset(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, dataset_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1_000\u001b[39m, str_len\u001b[38;5;241m=\u001b[39mparantheses_size)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining the model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain(train_dataset, val_dataset)\n\u001b[1;32m     24\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate_dataset(test_dataset)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Test Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 31\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_dataset, val_dataset)\u001b[0m\n\u001b[1;32m     29\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_dataloader))\n\u001b[1;32m     30\u001b[0m strings, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mtokenize_string_batch(strings)\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     34\u001b[0m loss, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_batch_loss_acc(x, y)\n",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m, in \u001b[0;36mTokenizer.tokenize_string_batch\u001b[0;34m(self, strings, add_cls_token)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m strings:\n\u001b[1;32m     26\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_string(s, add_cls_token\u001b[38;5;241m=\u001b[39madd_cls_token))\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(X, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got NoneType"
     ]
    }
   ],
   "source": [
    "run_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjNEOPRMsGKR"
   },
   "source": [
    "# Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjRY9u_UsFNm"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def seed_all():\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "class TransformerUnitTest:\n",
    "    def __init__(self, gt_vars: dict, verbose=False):\n",
    "        self.gt_vars = gt_vars\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def test_all(self):\n",
    "        self.test_tokenizer()\n",
    "        self.test_ape()\n",
    "        self.test_mha()\n",
    "        self.test_transformer_layer()\n",
    "        self.test_transformer_model()\n",
    "        self.test_scheduler()\n",
    "        self.test_loss()\n",
    "\n",
    "    def test_tokenizer(self):\n",
    "        seed_all()\n",
    "        self.check_correctness(\n",
    "            Tokenizer().tokenize_string('ccpeen', add_cls_token=True),\n",
    "            self.gt_vars['tokenizer_1'],\n",
    "            \"Tokenization with cls class\"\n",
    "        )\n",
    "        self.check_correctness(\n",
    "            Tokenizer().tokenize_string('cpppencpen', add_cls_token=False),\n",
    "            self.gt_vars['tokenizer_2'],\n",
    "            \"Tokenization without cls class\"\n",
    "        )\n",
    "\n",
    "    def test_ape(self):\n",
    "        seed_all()\n",
    "        ape_result = AbsolutePositionalEncoding(128)(torch.randn((8, 12, 128)))\n",
    "        self.check_correctness(ape_result, self.gt_vars['ape'], \"APE\")\n",
    "\n",
    "    def test_mha(self):\n",
    "        seed_all()\n",
    "        mha_result = MultiHeadAttention(d_model=128, n_heads=4, rpe=False)(\n",
    "            torch.randn((8, 12, 128)), torch.randn((8, 12, 128)), torch.randn((8, 12, 128))\n",
    "        )\n",
    "        self.check_correctness(\n",
    "            mha_result,\n",
    "            self.gt_vars['mha_no_rpe'],\n",
    "            \"Multi-head Attention without RPE\"\n",
    "        )\n",
    "        mha_result_rpe = MultiHeadAttention(d_model=128, n_heads=8, rpe=True)(\n",
    "            torch.randn((8, 12, 128)), torch.randn((8, 12, 128)), torch.randn((8, 12, 128))\n",
    "        )\n",
    "        self.check_correctness(\n",
    "            mha_result_rpe,\n",
    "            self.gt_vars['mha_with_rpe'],\n",
    "            \"Multi-head Attention with RPE\"\n",
    "        )\n",
    "\n",
    "    def test_transformer_layer(self):\n",
    "        seed_all()\n",
    "        for prenorm in [True, False]:\n",
    "            transformer_layer_result = TransformerLayer(\n",
    "                d_model=128, n_heads=4, prenorm=prenorm, rpe=False\n",
    "            )(torch.randn((8, 12, 128)))\n",
    "            self.check_correctness(\n",
    "                transformer_layer_result,\n",
    "                self.gt_vars[f'transformer_layer_prenorm_{prenorm}'],\n",
    "                f\"Transformer Layer Prenorm {prenorm}\"\n",
    "            )\n",
    "\n",
    "    def test_transformer_model(self):\n",
    "        seed_all()\n",
    "        transformer_model_result = TransformerModel(\n",
    "            ModelConfig(d_model=128, prenorm=True, pos_enc_type='ape')\n",
    "        )(torch.randn((8, 12, 5)))\n",
    "        self.check_correctness(\n",
    "            transformer_model_result,\n",
    "            self.gt_vars['transformer_model_result'],\n",
    "            f\"Transformer Model\"\n",
    "        )\n",
    "\n",
    "    def test_scheduler(self):\n",
    "        model = TransformerModel(ModelConfig())\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        scheduler = CustomScheduler(optimizer, 10_000)\n",
    "        optimizer.step()\n",
    "        scheduler.step(521)\n",
    "        self.check_correctness(\n",
    "            torch.tensor([optimizer.param_groups[0]['lr']]),\n",
    "            self.gt_vars['scheduler_1'],\n",
    "            f\"Scheduler Warmup\"\n",
    "        )\n",
    "        scheduler.step(2503)\n",
    "        self.check_correctness(\n",
    "            torch.tensor([optimizer.param_groups[0]['lr']]),\n",
    "            self.gt_vars['scheduler_2'],\n",
    "            f\"Scheduler Cooldown\"\n",
    "        )\n",
    "\n",
    "    def test_loss(self):\n",
    "        seed_all()\n",
    "        model = TransformerModel(ModelConfig())\n",
    "        trainer = Trainer(model, TrainerConfig(device='cpu'))\n",
    "        loss_result, _ = trainer.compute_batch_loss_acc(\n",
    "            torch.randn((8, 12, 5)),\n",
    "            torch.ones(8).float(),\n",
    "        )\n",
    "        self.check_correctness(\n",
    "            loss_result,\n",
    "            self.gt_vars['loss'],\n",
    "            f\"Batch Loss\"\n",
    "        )\n",
    "\n",
    "    def check_correctness(self, out, gt, title):\n",
    "        try:\n",
    "            diff = (out - gt).norm()\n",
    "        except:\n",
    "            diff = float('inf')\n",
    "        if diff < 1e-4:\n",
    "            print(f\"[Correct] {title}\")\n",
    "        else:\n",
    "            print(f\"[Wrong] {title}\")\n",
    "            if self.verbose:\n",
    "                print(\"-----\")\n",
    "                print(\"Expected: \")\n",
    "                print(gt)\n",
    "                print(\"Received: \")\n",
    "                print(out)\n",
    "                print(\"-----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2DlMVJ4wMrp"
   },
   "outputs": [],
   "source": [
    "!gdown 1-2-__6AALEfqhfew3sJ2QiCE1-rrFMnQ -q -O unit_tests.pkl\n",
    "import pickle\n",
    "with open('unit_tests.pkl', 'rb') as f:\n",
    "    gt_vars = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Enp2ArbjOHEt"
   },
   "outputs": [],
   "source": [
    "TransformerUnitTest(gt_vars, verbose=False).test_all()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "17R4gP0W6ojTJkVj4CvyCL8OfL4023-T2",
     "timestamp": 1704355227011
    },
    {
     "file_id": "1ERuLIkcLajusj-j83XOJ4Krj_E61p5pm",
     "timestamp": 1678522819080
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
